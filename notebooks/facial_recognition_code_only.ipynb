{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import keras\n",
    "import sklearn\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppr_models():\n",
    "    !rm models.zip\n",
    "    !rm models -r\n",
    "\n",
    "def suppr_figures():\n",
    "    !rm figures.zip\n",
    "    !rm figures -r\n",
    "\n",
    "def suppr_data():\n",
    "    !rm data.zip\n",
    "    !rm data -r\n",
    "    !rm __MACOSX -r\n",
    "\n",
    "suppr_models()\n",
    "suppr_figures()\n",
    "suppr_data()\n",
    "\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n",
    "!unzip -q models.zip\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/figures.zip\n",
    "!unzip -q figures.zip\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat')\n",
    "\n",
    "def face_locations(image, model=\"hog\"):\n",
    "\n",
    "    if model == \"hog\":\n",
    "        detector = hog_detector\n",
    "        cst = 0\n",
    "    elif model == \"cnn\":\n",
    "        detector = cnn_detector\n",
    "        cst = 10\n",
    "\n",
    "    matches = detector(image,1)\n",
    "    rects   = []\n",
    "\n",
    "    for r in matches:\n",
    "        if model == \"cnn\":\n",
    "            r = r.rect\n",
    "        x = max(r.left(), 0)\n",
    "        y = max(r.top(), 0)\n",
    "        w = min(r.right(), image.shape[1]) - x + cst\n",
    "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
    "        rects.append((x,y,w,h))\n",
    "\n",
    "    return rects\n",
    "\n",
    "def extract_faces(image, model=\"hog\"):\n",
    "\n",
    "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = face_locations(gray, model)\n",
    "    faces = []\n",
    "\n",
    "    for (x,y,w,h) in rects:\n",
    "        cropped = image[y:y+h, x:x+w, :]\n",
    "        cropped = cv2.resize(cropped, (128,128))\n",
    "        faces.append(cropped)\n",
    "\n",
    "    return faces\n",
    "\n",
    "def show_grid(faces, figsize=(12,3)):\n",
    "\n",
    "    n = len(faces)\n",
    "    cols = 7\n",
    "    rows = int(np.ceil(n/cols))\n",
    "\n",
    "    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            i = r*cols + c\n",
    "            if i == n:\n",
    "                 break\n",
    "            ax[r,c].imshow(faces[i])\n",
    "            ax[r,c].axis('off')\n",
    "            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))\n",
    "\n",
    "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
    "\n",
    "    imagePaths = []\n",
    "\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
    "                imagePaths.append(imagePath)\n",
    "\n",
    "    return imagePaths\n",
    "\n",
    "base_dir = \"data\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"figures/faces.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(image)\n",
    "\n",
    "faces = extract_faces(image, \"cnn\")  # Replace 'cnn' with 'hog' for faster but less accurate results\n",
    "show_grid(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation des données\n",
    "\n",
    "!ls data/alan_grant/\n",
    "# Visualisation d'une image des données pour se faire une idée\n",
    "impath = \"data/alan_grant/00000082.jpg\"\n",
    "if os.path.exists(impath):\n",
    "    image = cv2.imread(impath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extraction des faces dans les mêmes répertoires data/prenom_nom\n",
    "\n",
    "# Liste tous les chemins d'images dans le répertoire \"data\"\n",
    "imagePaths = list_images(\"data\")\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    # Si le nom de l'image contient le mot \"face\", on passe à l'image suivante\n",
    "    if \"face\" in os.path.basename(imagePath): \n",
    "        continue\n",
    "\n",
    "    # Lecture de l'image à partir du chemin\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    # Extraction des visages de l'image\n",
    "    # Utilisation du modèle \"cnn\" pour une meilleure détection de face, bien que \"hog\" soit plus rapide \n",
    "    faces = extract_faces(image, model=\"cnn\")\n",
    "\n",
    "    # Si aucun visage n'est détecté, on passe à l'image suivante\n",
    "    if len(faces) == 0:\n",
    "        continue\n",
    "\n",
    "    # Comme il n'y a jamais plus d'un visage par image, on manipule directement le premier visage détectée\n",
    "    face = faces[0]\n",
    "\n",
    "    # Récupération du chemin complet en remplaçant l'extension (.png, .jpg...) par \"_face.jpg\"\n",
    "    face_filename = os.path.splitext(imagePath)[0] + \"_face.jpg\"\n",
    "\n",
    "    # Enregistrement du visage dans le même répertoire que l'image d'origine\n",
    "    cv2.imwrite(face_filename, face)\n",
    "\n",
    "print(\"Extraction des visages terminée.\")\n",
    "\n",
    "\n",
    "# 2. Réarrangement des visages dans des répertoires d'entraînement, de validation et de test\n",
    "\n",
    "# Création des répertoires\n",
    "for path in [train_dir, val_dir, test_dir]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path) # suppression des répertoires déjà existants\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Récupération des répertoires prenom_nom dans data\n",
    "person_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))\n",
    "               and d not in [\"train\", \"validation\", \"test\"]]\n",
    "\n",
    "for person in person_dirs:\n",
    "    person_path = os.path.join(base_dir, person)\n",
    "\n",
    "    # Utilisation de list_images pour récupérer les images qui terminent par \"face.jpg\"\n",
    "    images = list_images(person_path, contains=\"_face.jpg\")\n",
    "\n",
    "    # Mélange aléatoire des images\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calcul des indices de découpage pour répartir les images en trois ensembles\n",
    "    total = len(images)\n",
    "    train_count = int(total * 0.65)  # % des images dans les données d'entraînement\n",
    "    val_count = int(total * 0.2)  # % des images pour la validation\n",
    "\n",
    "    # Découpage de l'ensemble des images en trois groupes : entraînement, validation et test\n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Définition des chemins de destination pour chaque ensemble\n",
    "    person_train_dir = os.path.join(train_dir, person)\n",
    "    person_val_dir = os.path.join(val_dir, person)\n",
    "    person_test_dir = os.path.join(test_dir, person)\n",
    "\n",
    "    # Création des répertoires de destination s'ils n'existent pas déjà\n",
    "    for path in [person_train_dir, person_val_dir, person_test_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Déplacement des images dans les bons répertoires\n",
    "    for img in train_images:\n",
    "        shutil.move(img, os.path.join(person_train_dir, os.path.basename(img)))\n",
    "    for img in val_images:\n",
    "        shutil.move(img, os.path.join(person_val_dir, os.path.basename(img)))\n",
    "    for img in test_images:\n",
    "        shutil.move(img, os.path.join(person_test_dir, os.path.basename(img)))\n",
    "\n",
    "print(\"Répartition des visages dans les répertoires train, validation et test terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des générateurs\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=10,\n",
    "    class_mode='categorical' # categorical puisqu'on a 6 variables catégorielles (les différents personnages)\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=10,          # Ajusté en f° du nombre d'images dans les données de validation\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=10,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('data label shape:', labels_batch.shape)\n",
    "\n",
    "    plt.imshow(data_batch[0])\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création et entraînement du réseau de neurones\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax')) # softmax vu qu'on a 6 sorties\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,  # Use train_generator for training data\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator  # Use validation_generator for validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training info\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "\n",
    "# Visualize the history plots\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b', label='Training loss')\n",
    "plt.plot(val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b', label='Training acc')\n",
    "plt.plot(val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(validation_generator, steps=20)\n",
    "print('Validation accuracy: {:2.2f}%'.format(val_acc*100))\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=20)\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
    "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n",
    "\n",
    "def face_landmarks(face, model=\"large\"):\n",
    "\n",
    "    if model == \"large\":\n",
    "        predictor = pose68\n",
    "    elif model == \"small\":\n",
    "        predictor = pose05\n",
    "\n",
    "    if not isinstance(face, list):\n",
    "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
    "        return predictor(face, rect)\n",
    "    else:\n",
    "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
    "        return [predictor(f,rect) for f in face]\n",
    "\n",
    "def shape_to_coords(shape):\n",
    "    return np.float32([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
    "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])\n",
    "\n",
    "\n",
    "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
    "    faces = []\n",
    "    for (img, marks) in zip(images, landmarks):\n",
    "        imgDim = img.shape[0]\n",
    "        coords = shape_to_coords(marks)\n",
    "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
    "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
    "        faces.append(warped)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = face_landmarks(faces)\n",
    "\n",
    "new_faces = []\n",
    "for (face,shape) in zip(faces, landmarks):\n",
    "    canvas = face.copy()\n",
    "    coords = shape_to_coords(shape)\n",
    "    for p in coords:\n",
    "        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n",
    "    new_faces.append(canvas)\n",
    "\n",
    "show_grid(new_faces, figsize=(15,5))\n",
    "\n",
    "aligned = align_faces(faces, landmarks)\n",
    "show_grid(aligned, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( np.stack(aligned, axis=3).astype(np.float32).mean(axis=3)/255 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de chaque visages du répertoire data en visages centrés\n",
    "# Chaque visage est traité individuellement\n",
    "def center_faces(base_dir):\n",
    "    # Lister toutes les images terminant par \"_face.jpg\"\n",
    "    imagePaths = list_images(base_dir, contains=\"_face.jpg\")\n",
    "\n",
    "    for imagePath in imagePaths:\n",
    "        # Lire l'image\n",
    "        image = cv2.imread(imagePath)\n",
    "\n",
    "        # Détecter les points de repère\n",
    "        landmarks = face_landmarks(image)\n",
    "\n",
    "        # Aligner le visage\n",
    "        aligned_face = align_faces([image], [landmarks])[0]\n",
    "\n",
    "        # Construire le chemin pour sauvegarder l'image centrée\n",
    "        newImagePath = imagePath.replace(\"_face.jpg\", \"_centered.jpg\")\n",
    "\n",
    "        # Sauvegarder l'image centrée\n",
    "        cv2.imwrite(newImagePath, aligned_face)\n",
    "\n",
    "        # Supprimer l'image originale (_face.jpg)\n",
    "        os.remove(imagePath)\n",
    "        \n",
    "    print(\"Traitement des images terminé.\")\n",
    "\n",
    "center_faces(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/train/alan_grant/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('data label shape:', labels_batch.shape)\n",
    "\n",
    "    plt.imshow(data_batch[0])\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,  # Use train_generator for training data\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator  # Use validation_generator for validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training info\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "\n",
    "# Visualize the history plots\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b', label='Training loss')\n",
    "plt.plot(val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b', label='Training acc')\n",
    "plt.plot(val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss, validation_accuracy = model.evaluate(validation_generator)\n",
    "print('Validation accuracy: {:2.2f}%'.format(validation_accuracy * 100))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_encoder = dlib.face_recognition_model_v1('models/dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "def face_encoder(faces):\n",
    "\n",
    "    landmarks = face_landmarks(faces)\n",
    "\n",
    "    if not isinstance(faces, list):\n",
    "        return np.array(cnn_encoder.compute_face_descriptor(faces,landmarks))\n",
    "    else:\n",
    "        return np.array([cnn_encoder.compute_face_descriptor(f,l) for f,l in zip(faces,landmarks)])\n",
    "\n",
    "\n",
    "encoded_faces = face_encoder(faces)\n",
    "\n",
    "plt.plot(encoded_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_faces(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Lister tous les sous-répertoires dans le répertoire donné\n",
    "    for person_dir in os.listdir(directory):\n",
    "        person_path = os.path.join(directory, person_dir)\n",
    "\n",
    "        # Lister toutes les images terminant par \"_centered.jpg\"\n",
    "        image_paths = [os.path.join(person_path, file_name) for file_name in os.listdir(person_path) if file_name.endswith('_centered.jpg')]\n",
    "        # Lire toutes les images\n",
    "        images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "\n",
    "        # Encoder les visages\n",
    "        encoded_faces = face_encoder(images)\n",
    "\n",
    "        # Ajouter les vecteurs encodés et les labels aux listes\n",
    "        data.extend(encoded_faces)\n",
    "        labels.extend([person_dir] * len(encoded_faces))\n",
    "\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "train_data, train_labels = encode_faces(train_dir)\n",
    "val_data, val_labels = encode_faces(val_dir)\n",
    "test_data, test_labels = encode_faces(test_dir)\n",
    "\n",
    "# Encodage one-hot des labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "train_labels_onehot = label_binarizer.fit_transform(train_labels)\n",
    "val_labels_onehot = label_binarizer.transform(val_labels)\n",
    "test_labels_onehot = label_binarizer.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du réseau\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(128,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(train_labels_onehot.shape[1], activation='softmax')) # REMARK: softmax is for multi-class classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels_onehot, validation_data=(val_data, val_labels_onehot), epochs=40, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training info\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc      = history.history['accuracy']\n",
    "val_acc  = history.history['val_accuracy']\n",
    "\n",
    "# Visualize the history plots\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b', label='Training loss')\n",
    "plt.plot(val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b', label='Training acc')\n",
    "plt.plot(val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "val_loss, val_accuracy = model.evaluate(val_data, val_labels_onehot)\n",
    "print('Validation accuracy: {:2.2f}%'.format(val_accuracy * 100))\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels_onehot)\n",
    "print('Test accuracy: {:2.2f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(image, mode=\"fast\", model=None):\n",
    "\n",
    "    # face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if mode == \"fast\":\n",
    "        matches = hog_detector(gray,1)\n",
    "    else:\n",
    "        matches = cnn_detector(gray,1)\n",
    "        matches = [m.rect for m in matches]\n",
    "\n",
    "    for rect in matches:\n",
    "\n",
    "        # face classification\n",
    "        if model is None:\n",
    "            label = \"label\"\n",
    "        else:\n",
    "            # face landmarks\n",
    "            landmarks = pose68(gray, rect)\n",
    "             # face encoding\n",
    "            encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
    "            # Convert the encoding to the correct shape for prediction\n",
    "            encoding = np.array(encoding).reshape(1, -1)\n",
    "            # Predict the label\n",
    "            prediction = model.predict(encoding)\n",
    "            label = prediction[0]\n",
    "        \n",
    "        # draw box\n",
    "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
    "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "def process_movie(video_name, outvideo_name='/kaggle/working/videoResult.mp4', mode=\"fast\", model=None):\n",
    "\n",
    "    video  = cv2.VideoCapture(video_name)\n",
    "    if (video.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "        return\n",
    "    \n",
    "    frame_width = int(video.get(3))\n",
    "    frame_height = int(video.get(4))\n",
    "    out_mp4 = cv2.VideoWriter(outvideo_name,cv2.VideoWriter_fourcc(*'XVID'), 10, (frame_width//2,frame_height//2))\n",
    "\n",
    "    i=0\n",
    "    while video.isOpened():\n",
    "\n",
    "        # Grab a single frame of video\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            # Resize frame of video for faster processing\n",
    "            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "            # Process frame\n",
    "            image = process_frame(frame, mode, model)\n",
    "            # Write the processed frame to output video\n",
    "            out_mp4.write(image)\n",
    "        else:\n",
    "            break\n",
    "        i += 1\n",
    "        if i==1000:\n",
    "            break\n",
    "    # Release video\n",
    "    video.release()\n",
    "    out_mp4.release()\n",
    "    print(\"Video released\")\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play(filename):\n",
    "    html = ''\n",
    "    video = open(filename,'rb').read()\n",
    "    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n",
    "    html += '<video width=1000 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n",
    "    return HTML(html)\n",
    "\n",
    "\n",
    "def suppr_test():\n",
    "    !rm test.zip\n",
    "    !rm test -r\n",
    "    !rm __MACOSX -r\n",
    "\n",
    "suppr_test()\n",
    "\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/test.zip\n",
    "!unzip -q test.zip\n",
    "!ls test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(train_data, train_labels)\n",
    "\n",
    "# SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(train_data, train_labels)\n",
    "\n",
    "# kNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=15)\n",
    "knn_model.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÉVALUATION DES MODÈLES\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, train_data, train_labels, test_data, test_labels):\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(test_data)\n",
    "    prediction_time = time.time() - start_time\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    return accuracy, prediction_time\n",
    "\n",
    "def evaluate_keras_model(model, train_data, train_labels, test_data, test_labels):\n",
    "    start_time = time.time()\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    prediction_time = time.time() - start_time\n",
    "    return accuracy, prediction_time\n",
    "\n",
    "logistic_accuracy, logistic_pred_time = evaluate_model(logistic_model, train_data, train_labels, test_data, test_labels)\n",
    "print(f'Logistic Regression - Accuracy: {logistic_accuracy:.2f}, Prediction Time: {logistic_pred_time:.4f}s')\n",
    "\n",
    "svm_accuracy, svm_pred_time = evaluate_model(svm_model, train_data, train_labels, test_data, test_labels)\n",
    "print(f'SVM - Accuracy: {svm_accuracy:.2f}, Prediction Time: {svm_pred_time:.4f}s')\n",
    "\n",
    "knn_accuracy, knn_pred_time = evaluate_model(knn_model, train_data, train_labels, test_data, test_labels)\n",
    "print(f'kNN - Accuracy: {knn_accuracy:.2f}, Prediction Time: {knn_pred_time:.4f}s')\n",
    "\n",
    "nn_accuracy, nn_pred_time = evaluate_keras_model(model, train_data, train_labels_onehot, test_data, test_labels_onehot)\n",
    "print(f'Neural Network - Accuracy: {nn_accuracy:.2f}, Prediction Time: {nn_pred_time:.4f}s')\n",
    "# model : réseau entraîné dans la partie précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur une des images fournie\n",
    "image = cv2.imread(\"test/example_03.png\")\n",
    "processed = process_frame(image.copy(), model=logistic_model)\n",
    "processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur la vidéo fournie\n",
    "!rm videoResult.mp4\n",
    "!rm videoResult_fixed.mp4\n",
    "process_movie(\"test/lunch_scene.mp4\", mode=\"not fast\", model=logistic_model)\n",
    "# Reconvertir la vidéo avec ffmpeg en .mp4 compatible\n",
    "!ffmpeg -i videoResult.mp4 -vcodec libx264 -acodec aac videoResult_fixed.mp4 >/dev/null 2>&1\n",
    "play('/kaggle/working/videoResult_fixed.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette partie peut être exécutée indépendamment des parties pécédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import keras\n",
    "import sklearn\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppr_models():\n",
    "    !rm models.zip\n",
    "    !rm models -r\n",
    "\n",
    "def suppr_figures():\n",
    "    !rm figures.zip\n",
    "    !rm figures -r\n",
    "\n",
    "def suppr_data():\n",
    "    !rm data.zip\n",
    "    !rm data -r\n",
    "    !rm __MACOSX -r\n",
    "\n",
    "suppr_models()\n",
    "suppr_figures()\n",
    "suppr_data()\n",
    "\n",
    "# importation des modèles préentraînés fournis\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n",
    "!unzip -q models.zip\n",
    "\n",
    "\n",
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat') # nécessite un GPU\n",
    "\n",
    "# fonctions utilitaires\n",
    "def face_locations(image, model=\"hog\"):\n",
    "\n",
    "    if model == \"hog\":\n",
    "        detector = hog_detector\n",
    "        cst = 0\n",
    "    elif model == \"cnn\":\n",
    "        detector = cnn_detector\n",
    "        cst = 10\n",
    "\n",
    "    matches = detector(image,1)\n",
    "    rects   = []\n",
    "\n",
    "    for r in matches:\n",
    "        if model == \"cnn\":\n",
    "            r = r.rect\n",
    "        x = max(r.left(), 0)\n",
    "        y = max(r.top(), 0)\n",
    "        w = min(r.right(), image.shape[1]) - x + cst\n",
    "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
    "        rects.append((x,y,w,h))\n",
    "\n",
    "    return rects\n",
    "\n",
    "def extract_faces(image, model=\"hog\"):\n",
    "\n",
    "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = face_locations(gray, model)\n",
    "    faces = []\n",
    "\n",
    "    for (x,y,w,h) in rects:\n",
    "        cropped = image[y:y+h, x:x+w, :]\n",
    "        cropped = cv2.resize(cropped, (128,128))\n",
    "        faces.append(cropped)\n",
    "\n",
    "    return faces\n",
    "\n",
    "def show_grid(faces, figsize=(12,3)):\n",
    "\n",
    "    n = len(faces)\n",
    "    cols = 7\n",
    "    rows = int(np.ceil(n/cols))\n",
    "\n",
    "    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            i = r*cols + c\n",
    "            if i == n:\n",
    "                 break\n",
    "            ax[r,c].imshow(faces[i])\n",
    "            ax[r,c].axis('off')\n",
    "            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))\n",
    "\n",
    "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
    "\n",
    "    imagePaths = []\n",
    "\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
    "                imagePaths.append(imagePath)\n",
    "\n",
    "    return imagePaths\n",
    "\n",
    "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
    "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n",
    "\n",
    "def face_landmarks(face, model=\"large\"):\n",
    "\n",
    "    if model == \"large\":\n",
    "        predictor = pose68\n",
    "    elif model == \"small\":\n",
    "        predictor = pose05\n",
    "\n",
    "    if not isinstance(face, list):\n",
    "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
    "        return predictor(face, rect)\n",
    "    else:\n",
    "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
    "        return [predictor(f,rect) for f in face]\n",
    "\n",
    "def shape_to_coords(shape):\n",
    "    return np.float32([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
    "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])\n",
    "\n",
    "\n",
    "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
    "    faces = []\n",
    "    for (img, marks) in zip(images, landmarks):\n",
    "        imgDim = img.shape[0]\n",
    "        coords = shape_to_coords(marks)\n",
    "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
    "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
    "        faces.append(warped)\n",
    "    return faces\n",
    "\n",
    "\n",
    "base_dir = \"data\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Au choix :\n",
    "\n",
    "# importation du jeu de données brut\n",
    "!pip install -q gdown\n",
    "import gdown\n",
    "\n",
    "file_id = \"1I6GZo2uU3d6r51pzbfmJlE7LhQ-R2GKr\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "gdown.download(url, output=\"data.zip\", quiet=False)\n",
    "\n",
    "!unzip -q data.zip\n",
    "\n",
    "# Extraction des visages dans les mêmes répertoires data/prenom_nom\n",
    "\n",
    "imagePaths = list_images(\"data\")\n",
    "for imagePath in imagePaths:\n",
    "    if \"face\" in os.path.basename(imagePath): \n",
    "        continue\n",
    "    image = cv2.imread(imagePath)\n",
    "    if image is None:\n",
    "        print(f\"Erreur : Impossible de lire l'image {imagePath}. Elle sera ignorée.\")\n",
    "        continue\n",
    "    faces = extract_faces(image, model=\"cnn\")\n",
    "    if len(faces) == 0:\n",
    "        print(f\"Erreur : Aucun visage détecté dans l'image {imagePath}. Elle sera ignorée.\")\n",
    "        continue\n",
    "    if len(faces) > 1:\n",
    "        print(f\"Erreur : Plus d'un visage détecté dans l'image {imagePath}. Elle sera ignorée.\")\n",
    "        continue\n",
    "    face = faces[0]\n",
    "    face_filename = os.path.splitext(imagePath)[0] + \"_face.jpg\"\n",
    "    cv2.imwrite(face_filename, face)\n",
    "print(\"Extraction des visages terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ou bien :\n",
    "# Charger les visages déjà extraits (pour gagner du temps)\n",
    "suppr_data()\n",
    "\n",
    "!pip install -q gdown\n",
    "import gdown\n",
    "file_id = \"1OXezr6FgQIitVeKSzHaqtr4WUIQfOwIM\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "gdown.download(url, output=\"data_faces.zip\", quiet=False)\n",
    "!unzip -q data_faces.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des visages dans des répertoires d'entraînement, de validation et de test\n",
    "\n",
    "# Création des répertoires\n",
    "for path in [train_dir, val_dir, test_dir]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path) # suppression des répertoires déjà existants\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "# Récupération des répertoires prenom_nom dans data\n",
    "person_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))\n",
    "               and d not in [\"train\", \"validation\", \"test\"]]\n",
    "\n",
    "for person in person_dirs:\n",
    "    person_path = os.path.join(base_dir, person)\n",
    "\n",
    "    # Utilisation de list_images pour récupérer les images qui terminent par \"face.jpg\"\n",
    "    images = list_images(person_path, contains=\"_face.jpg\")\n",
    "\n",
    "    # Mélange aléatoire des images\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calcul des indices de découpage pour répartir les images en trois ensembles\n",
    "    total = len(images)\n",
    "    train_count = int(total * 0.7)  # % des images dans les données d'entraînement\n",
    "    val_count = int(total * 0.15)  # % des images pour la validation\n",
    "\n",
    "    # Découpage de l'ensemble des images en trois groupes : entraînement, validation et test\n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Définition des chemins de destination pour chaque ensemble\n",
    "    person_train_dir = os.path.join(train_dir, person)\n",
    "    person_val_dir = os.path.join(val_dir, person)\n",
    "    person_test_dir = os.path.join(test_dir, person)\n",
    "\n",
    "    # Création des répertoires de destination s'ils n'existent pas déjà\n",
    "    for path in [person_train_dir, person_val_dir, person_test_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Déplacement des images dans les bons répertoires\n",
    "    for img in train_images:\n",
    "        shutil.move(img, os.path.join(person_train_dir, os.path.basename(img)))\n",
    "    for img in val_images:\n",
    "        shutil.move(img, os.path.join(person_val_dir, os.path.basename(img)))\n",
    "    for img in test_images:\n",
    "        shutil.move(img, os.path.join(person_test_dir, os.path.basename(img)))\n",
    "\n",
    "print(\"Répartition des visages dans les répertoires train, validation et test terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
