{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O56jp96I0MQo"
   },
   "source": [
    "# Assignment B.5 - Modern face recognition with deep learning\n",
    "\n",
    "Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic. This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing: these algorithms can recognize faces with 98% accuracy, which is pretty much as good as humans can do!\n",
    "\n",
    "As a human, your brain is wired to recognize faces automatically and instantly. Computers are not capable of doing this, so you have to teach them how to tackle each step in this process. Specifically, a face recognition system goes through four steps: find faces in the image, analyze their facial features, compare against known faces, and make a prediction of the corresponding persons. Here's described the full pipeline.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/summary.gif\" style=\"height:200px;\">\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "In this assignment, you will tackle several problems related to face recognition:\n",
    "1. **Face detection**. Look at a picture and find all the faces in it. -5\n",
    "- **Pose estimation**. Understand where the face is turned and correct its pose. 5\n",
    "- **Face encoding**. Pick up unique features from a face that can be used to distinguish it from others. 5\n",
    "- **Face recognition**. Compare the unique features of a face to those of all the people in a database. 5\n",
    "- **Personal dataset**. Build a custom face recognition dataset. 2\n",
    "\n",
    "By the end of this notebook, you will have your own face recognition system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxS9du380MQw"
   },
   "source": [
    "### Required packages\n",
    "\n",
    "Here are the packages you will need during the assignment.\n",
    "- [Numpy](http://www.numpy.org)\n",
    "- [Keras](https://keras.io)\n",
    "- [OpenCV](https://opencv.org)\n",
    "- [Dlib](http://dlib.net).\n",
    "\n",
    "**Note**: In Anaconda Navigator, the package `dlib` can be installed from the **conda-forge** channel. In Google Colab, everything is readily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy3onABl0MQ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import keras\n",
    "import sklearn\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnCc8P7y0MQ1"
   },
   "source": [
    "## 1. Face detection\n",
    "\n",
    "The first step in your pipeline is face detection. Obviously you need to locate the faces in a photograph before you can try to tell them apart. If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action. Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But you’ll use it for a different purpose:  finding the areas of the image you want to pass on to the next step in your pipeline.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/detection.jpg\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A5esp2s0MQ2"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- You are provided with a small dataset of pictures, where each picture contains exactly one face. Extract the faces and their labels (i.e., the person's names). Store them to a new file with the function `dump` in the package `pickle`.\n",
    "\n",
    "\n",
    "-  Normalize the cropped faces (i.e., divide the pixel values by 255), and split them in train set (70%) and test set (30%) with the function `train_test_split` in the package `sklearn`.\n",
    "\n",
    "\n",
    "- Train a small convnet and check its performance on the test set. Remember: don't use the test images for training.\n",
    "\n",
    "\n",
    "- Try to improve the performance of the baseline convnet by using all the tricks you have learned in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfqaWaKR0MQ3"
   },
   "source": [
    "#### Provided functions\n",
    "\n",
    "Here you will find some useful functions to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppr_models():\n",
    "    !rm models.zip\n",
    "    !rm models -r\n",
    "\n",
    "def suppr_figures():\n",
    "    !rm figures.zip\n",
    "    !rm figures -r\n",
    "\n",
    "def suppr_data():\n",
    "    !rm data.zip\n",
    "    !rm data -r\n",
    "    !rm __MACOSX -r\n",
    "\n",
    "suppr_models()\n",
    "suppr_figures()\n",
    "suppr_data()\n",
    "\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n",
    "!unzip -q models.zip\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/figures.zip\n",
    "!unzip -q figures.zip\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip\n",
    "!unzip -q data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RW7kXCj0MQ4"
   },
   "outputs": [],
   "source": [
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat')\n",
    "\n",
    "def face_locations(image, model=\"hog\"):\n",
    "\n",
    "    if model == \"hog\":\n",
    "        detector = hog_detector\n",
    "        cst = 0\n",
    "    elif model == \"cnn\":\n",
    "        detector = cnn_detector\n",
    "        cst = 10\n",
    "\n",
    "    matches = detector(image,1)\n",
    "    rects   = []\n",
    "\n",
    "    for r in matches:\n",
    "        if model == \"cnn\":\n",
    "            r = r.rect\n",
    "        x = max(r.left(), 0)\n",
    "        y = max(r.top(), 0)\n",
    "        w = min(r.right(), image.shape[1]) - x + cst\n",
    "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
    "        rects.append((x,y,w,h))\n",
    "\n",
    "    return rects\n",
    "\n",
    "def extract_faces(image, model=\"hog\"):\n",
    "\n",
    "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = face_locations(gray, model)\n",
    "    faces = []\n",
    "\n",
    "    for (x,y,w,h) in rects:\n",
    "        cropped = image[y:y+h, x:x+w, :]\n",
    "        cropped = cv2.resize(cropped, (128,128))\n",
    "        faces.append(cropped)\n",
    "\n",
    "    return faces\n",
    "\n",
    "def show_grid(faces, figsize=(12,3)):\n",
    "\n",
    "    n = len(faces)\n",
    "    cols = 7\n",
    "    rows = int(np.ceil(n/cols))\n",
    "\n",
    "    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            i = r*cols + c\n",
    "            if i == n:\n",
    "                 break\n",
    "            ax[r,c].imshow(faces[i])\n",
    "            ax[r,c].axis('off')\n",
    "            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))\n",
    "\n",
    "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
    "\n",
    "    imagePaths = []\n",
    "\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
    "                imagePaths.append(imagePath)\n",
    "\n",
    "    return imagePaths\n",
    "\n",
    "base_dir = \"data\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMuqxVrJ0MQ9"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `extract_faces()` applies face detection to a single input image, and returns a list of 128x128 blocks containing the detected faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1484,
     "status": "ok",
     "timestamp": 1612372015155,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "AJtMFU3W0MQ-",
    "outputId": "ea0f9d9a-8aa3-4bd2-f58e-5c4280d40365"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(\"figures/faces.png\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(image)\n",
    "\n",
    "faces = extract_faces(image, \"cnn\")  # Replace 'cnn' with 'hog' for faster but less accurate results\n",
    "show_grid(faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfp7kZK10MQ_"
   },
   "source": [
    "Moreover, the function `list_images()` locates all the jpeg/png/tiff files in a given folder (including its subfolders)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE PARTIE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation des données\n",
    "\n",
    "!ls data/alan_grant/\n",
    "# Visualisation d'une image des données pour se faire une idée\n",
    "impath = \"data/alan_grant/00000082.jpg\"\n",
    "if os.path.exists(impath):\n",
    "    image = cv2.imread(impath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extraction des faces dans les mêmes répertoires data/prenom_nom\n",
    "\n",
    "# Liste tous les chemins d'images dans le répertoire \"data\"\n",
    "imagePaths = list_images(\"data\")\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    # Si le nom de l'image contient le mot \"face\", on passe à l'image suivante\n",
    "    if \"face\" in os.path.basename(imagePath): \n",
    "        continue\n",
    "\n",
    "    # Lecture de l'image à partir du chemin\n",
    "    image = cv2.imread(imagePath)\n",
    "\n",
    "    # Extraction des visages de l'image\n",
    "    # Utilisation du modèle \"cnn\" pour une meilleure détection de face, bien que \"hog\" soit plus rapide \n",
    "    faces = extract_faces(image, model=\"cnn\")\n",
    "\n",
    "    # Si aucun visage n'est détecté, on passe à l'image suivante\n",
    "    if len(faces) == 0:\n",
    "        continue\n",
    "\n",
    "    # Comme il n'y a jamais plus d'un visage par image, on manipule directement le premier visage détectée\n",
    "    face = faces[0]\n",
    "\n",
    "    # Récupération du chemin complet en remplaçant l'extension (.png, .jpg...) par \"_face.jpg\"\n",
    "    face_filename = os.path.splitext(imagePath)[0] + \"_face.jpg\"\n",
    "\n",
    "    # Enregistrement du visage dans le même répertoire que l'image d'origine\n",
    "    cv2.imwrite(face_filename, face)\n",
    "\n",
    "print(\"Extraction des visages terminée.\")\n",
    "\n",
    "\n",
    "# 2. Réarrangement des visages dans des répertoires d'entraînement, de validation et de test\n",
    "\n",
    "# Création des répertoires\n",
    "for path in [train_dir, val_dir, test_dir]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path) # suppression des répertoires déjà existants\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Récupération des répertoires prenom_nom dans data\n",
    "person_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))\n",
    "               and d not in [\"train\", \"validation\", \"test\"]]\n",
    "\n",
    "for person in person_dirs:\n",
    "    person_path = os.path.join(base_dir, person)\n",
    "\n",
    "    # Utilisation de list_images pour récupérer les images qui terminent par \"face.jpg\"\n",
    "    images = list_images(person_path, contains=\"_face.jpg\")\n",
    "\n",
    "    # Mélange aléatoire des images\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calcul des indices de découpage pour répartir les images en trois ensembles\n",
    "    total = len(images)\n",
    "    train_count = int(total * 0.65)  # % des images dans les données d'entraînement\n",
    "    val_count = int(total * 0.2)  # % des images pour la validation\n",
    "\n",
    "    # Découpage de l'ensemble des images en trois groupes : entraînement, validation et test\n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Définition des chemins de destination pour chaque ensemble\n",
    "    person_train_dir = os.path.join(train_dir, person)\n",
    "    person_val_dir = os.path.join(val_dir, person)\n",
    "    person_test_dir = os.path.join(test_dir, person)\n",
    "\n",
    "    # Création des répertoires de destination s'ils n'existent pas déjà\n",
    "    for path in [person_train_dir, person_val_dir, person_test_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Déplacement des images dans les bons répertoires\n",
    "    for img in train_images:\n",
    "        shutil.move(img, os.path.join(person_train_dir, os.path.basename(img)))\n",
    "    for img in val_images:\n",
    "        shutil.move(img, os.path.join(person_val_dir, os.path.basename(img)))\n",
    "    for img in test_images:\n",
    "        shutil.move(img, os.path.join(person_test_dir, os.path.basename(img)))\n",
    "\n",
    "print(\"Répartition des visages dans les répertoires train, validation et test terminée.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des générateurs\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=10,\n",
    "    class_mode='categorical' # categorical puisqu'on a 6 variables catégorielles (les différents personnages)\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=10,          # Ajusté en f° du nombre d'images dans les données de validation\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=10,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('data label shape:', labels_batch.shape)\n",
    "\n",
    "    plt.imshow(data_batch[0])\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création et entraînement du réseau de neurones\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax')) # softmax vu qu'on a 6 sorties\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,  # Use train_generator for training data\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator  # Use validation_generator for validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training info\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "\n",
    "# Visualize the history plots\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b', label='Training loss')\n",
    "plt.plot(val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b', label='Training acc')\n",
    "plt.plot(val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(validation_generator, steps=20)\n",
    "print('Validation accuracy: {:2.2f}%'.format(val_acc*100))\n",
    "test_loss, test_acc = model.evaluate(test_generator, steps=20)\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : En premier lieu nous avons obtenu un résultat étonnant. Les performances du modèle sur l'ensemble de test sont systématiquement meilleures que sur l'ensemble de validation. Nous avons tenter de faire varier les % de répartition des images dans les différents ensemble, et changé le batch size. Finalement, il semblerait que le problème soit lié au fait que les images de test soient plus faciles que les images de validation. Nous avons donc ajouté un mélange aléatoire des images avant répartition pour résoudre le problème. Il n'est cependant pas garanti à 100% que les performances de test soient toujours inférieures aux performances de validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFrGlU2q0MRB"
   },
   "source": [
    "## 2. Pose estimation\n",
    "\n",
    "You have isolated the faces in our image. But now you have to deal with the problem that faces turned different directions look totally different to a computer. To account for this, you will try to warp each picture so that the eyes and lips are always in the same place in the image. More concretely, you are going to use an algorithm called face landmark estimation. The basic idea is to locate 68 specific points (called landmarks) that exist on every face:  the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then, you’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. This will make face recognition more accurate.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/pose.png\" style=\"height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLBTOO7F0MRB"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- Further preprocess the face pictures by correcting their pose. You should now have a dataset of cropped, aligned, and normalized faces.\n",
    "\n",
    "\n",
    "- Re-train your convnets on the modified dataset.\n",
    "\n",
    "\n",
    "- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEeUbQSn0MRB"
   },
   "source": [
    "#### Provided functions\n",
    "\n",
    "Here you will find some useful functions to complete the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgIWJ13c0MRB"
   },
   "outputs": [],
   "source": [
    "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
    "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n",
    "\n",
    "def face_landmarks(face, model=\"large\"):\n",
    "\n",
    "    if model == \"large\":\n",
    "        predictor = pose68\n",
    "    elif model == \"small\":\n",
    "        predictor = pose05\n",
    "\n",
    "    if not isinstance(face, list):\n",
    "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
    "        return predictor(face, rect)\n",
    "    else:\n",
    "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
    "        return [predictor(f,rect) for f in face]\n",
    "\n",
    "def shape_to_coords(shape):\n",
    "    return np.float32([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
    "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])\n",
    "\n",
    "\n",
    "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
    "    faces = []\n",
    "    for (img, marks) in zip(images, landmarks):\n",
    "        imgDim = img.shape[0]\n",
    "        coords = shape_to_coords(marks)\n",
    "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
    "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
    "        faces.append(warped)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5S7-ag80MRC"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `face_landmarks()` computes the landmarks for a list of cropped faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jItizFr0MRE"
   },
   "outputs": [],
   "source": [
    "landmarks = face_landmarks(faces)\n",
    "\n",
    "new_faces = []\n",
    "for (face,shape) in zip(faces, landmarks):\n",
    "    canvas = face.copy()\n",
    "    coords = shape_to_coords(shape)\n",
    "    for p in coords:\n",
    "        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n",
    "    new_faces.append(canvas)\n",
    "\n",
    "show_grid(new_faces, figsize=(15,5))\n",
    "\n",
    "aligned = align_faces(faces, landmarks)\n",
    "show_grid(aligned, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1612372120733,
     "user": {
      "displayName": "Laurent NAJMAN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64",
      "userId": "14324108496982386588"
     },
     "user_tz": -60
    },
    "id": "bfTSgga90MRH",
    "outputId": "79aa80d2-b1b4-4fc6-be9b-26a3c7c0af57"
   },
   "outputs": [],
   "source": [
    "plt.imshow( np.stack(aligned, axis=3).astype(np.float32).mean(axis=3)/255 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE PARTIE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de chaque visages du répertoire data en visages centrés\n",
    "# Chaque visage est traité individuellement\n",
    "def center_faces(base_dir):\n",
    "    # Lister toutes les images terminant par \"_face.jpg\"\n",
    "    imagePaths = list_images(base_dir, contains=\"_face.jpg\")\n",
    "\n",
    "    for imagePath in imagePaths:\n",
    "        # Lire l'image\n",
    "        image = cv2.imread(imagePath)\n",
    "\n",
    "        # Détecter les points de repère\n",
    "        landmarks = face_landmarks(image)\n",
    "\n",
    "        # Aligner le visage\n",
    "        aligned_face = align_faces([image], [landmarks])[0]\n",
    "\n",
    "        # Construire le chemin pour sauvegarder l'image centrée\n",
    "        newImagePath = imagePath.replace(\"_face.jpg\", \"_centered.jpg\")\n",
    "\n",
    "        # Sauvegarder l'image centrée\n",
    "        cv2.imwrite(newImagePath, aligned_face)\n",
    "\n",
    "        # Supprimer l'image originale (_face.jpg)\n",
    "        os.remove(imagePath)\n",
    "        \n",
    "    print(\"Traitement des images terminé.\")\n",
    "\n",
    "center_faces(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/train/alan_grant/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('data label shape:', labels_batch.shape)\n",
    "\n",
    "    plt.imshow(data_batch[0])\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,  # Use train_generator for training data\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator  # Use validation_generator for validation data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training info\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "\n",
    "# Visualize the history plots\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b', label='Training loss')\n",
    "plt.plot(val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b', label='Training acc')\n",
    "plt.plot(val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss, validation_accuracy = model.evaluate(validation_generator)\n",
    "print('Validation accuracy: {:2.2f}%'.format(validation_accuracy * 100))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy: {:2.2f}%'.format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCQgD33c0MRH"
   },
   "source": [
    "## 3. Face encoding\n",
    "\n",
    "The simplest approach to face recognition is to directly classify an unknown face with a convnet trained on your database of tagged people. Seems like a pretty good idea, right? There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly train such a big convnet. That would take way too long. What you need is a way to extract a few basic measurements from each face, which you can then use to quickly compare the unknown face with your database. For example, you might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. However, it turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n",
    "\n",
    "The solution is to train a convnet. But instead of training the network to classify pictures, it is trained to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: the picture of a known person, another picture of the same known person, and a picture of a totally different person. Then, the algorithm looks at the measurements currently generated for each of those three images. It tweaks the neural network slightly to make sure that the measurements generated for the same person are slightly closer, and the measurements for different persons are slightly further apart. After repeating this step millions of times for millions of images of thousands of different people, the convnet learns to generate 128 measurements for each person.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/triplet.png\" style=\"height:400px;\">\n",
    "\n",
    "This process of training a convnet to output face encodings requires a lot of data and computer power. Even with an expensive GPU, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Fortunately, the people at [OpenFace](https://cmusatyalab.github.io/openface/) already did this and they published several trained networks which you can directly use. So all you need to do is run your face images through their pre-trained network to get the 128 measurements for each face.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/encoding.png\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo7Hduwu0MRH"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- Preprocess the cropped faces by encoding them. You should now have a dataset of cropped and encoded faces.\n",
    "\n",
    "\n",
    "- Train a neural network on the modified dataset. Since the encoded faces are just 128-length vectors, **you don't need a convnet**. Use a regular neural network with a series of fully-connected layers.\n",
    "\n",
    "\n",
    "- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWu06RRG0MRH"
   },
   "source": [
    "#### Provided functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deJ2n5b00MRI"
   },
   "outputs": [],
   "source": [
    "cnn_encoder = dlib.face_recognition_model_v1('models/dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "def face_encoder(faces):\n",
    "\n",
    "    landmarks = face_landmarks(faces)\n",
    "\n",
    "    if not isinstance(faces, list):\n",
    "        return np.array(cnn_encoder.compute_face_descriptor(faces,landmarks))\n",
    "    else:\n",
    "        return np.array([cnn_encoder.compute_face_descriptor(f,l) for f,l in zip(faces,landmarks)])\n",
    "\n",
    "\n",
    "encoded_faces = face_encoder(faces)\n",
    "\n",
    "plt.plot(encoded_faces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ujJWBu0MRJ"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `face_encoder()` computes the encodings for a list of cropped faces. Alignment and normalization are handled internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE PARTIE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_faces(directory):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # Lister tous les sous-répertoires dans le répertoire donné\n",
    "    for person_dir in os.listdir(directory):\n",
    "        person_path = os.path.join(directory, person_dir)\n",
    "\n",
    "        # Lister toutes les images terminant par \"_centered.jpg\"\n",
    "        image_paths = [os.path.join(person_path, file_name) for file_name in os.listdir(person_path) if file_name.endswith('_centered.jpg')]\n",
    "        # Lire toutes les images\n",
    "        images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "\n",
    "        # Encoder les visages\n",
    "        encoded_faces = face_encoder(images)\n",
    "\n",
    "        # Ajouter les vecteurs encodés et les labels aux listes\n",
    "        data.extend(encoded_faces)\n",
    "        labels.extend([person_dir] * len(encoded_faces))\n",
    "\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "train_data, train_labels = encode_faces(train_dir)\n",
    "val_data, val_labels = encode_faces(val_dir)\n",
    "test_data, test_labels = encode_faces(test_dir)\n",
    "\n",
    "# Encodage one-hot des labels\n",
    "label_binarizer = LabelBinarizer()\n",
    "train_labels_onehot = label_binarizer.fit_transform(train_labels)\n",
    "val_labels_onehot = label_binarizer.transform(val_labels)\n",
    "test_labels_onehot = label_binarizer.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du réseau\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(128,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(train_labels_onehot.shape[1], activation='softmax')) # REMARK: softmax is for multi-class classification\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, train_labels_onehot, validation_data=(val_data, val_labels_onehot), epochs=40, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training info\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "acc      = history.history['accuracy']\n",
    "val_acc  = history.history['val_accuracy']\n",
    "\n",
    "# Visualize the history plots\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b', label='Training loss')\n",
    "plt.plot(val_loss, 'm', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b', label='Training acc')\n",
    "plt.plot(val_acc, 'm', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "val_loss, val_accuracy = model.evaluate(val_data, val_labels_onehot)\n",
    "print('Validation accuracy: {:2.2f}%'.format(val_accuracy * 100))\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels_onehot)\n",
    "print('Test accuracy: {:2.2f}%'.format(test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7x18Hp30MRK"
   },
   "source": [
    "## 4. Face recognition\n",
    "\n",
    "This last step is actually the easiest one in the whole process. All you have to do is find the person in your database of known people who has the closest measurements to some test image. You can do that by using any machine learning classification algorithm, such as neaural network (as you did in the previous section), logistic regression, SVM, nearest neighbours, etc. All you need to do is training a classifier that can take in the measurements from a new test image, and tells which known person is the closest match. Running this classifier must only take milliseconds, so that you can apply it to video sequences.\n",
    "\n",
    "<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/test.gif\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3IRpEzh0MRK"
   },
   "source": [
    "### Assignment\n",
    "\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "-  Train several classifiers (logistic regression, SVM, kNN, neural network) on the dataset of encoded faces (you can use the package `scikit-learn`).\n",
    "\n",
    "- Evaluate their performance on the test set, in terms of accuracy and speed.\n",
    "\n",
    "- Finally, run your best classifier on the test images and video available in the `test` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYHj2ZUZ0MRK"
   },
   "source": [
    "#### Provided functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMPHcAJI4diD"
   },
   "source": [
    "**Note:** cv2.VideoCapture does not work in Google Colab. You can use https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi to capture video 'on the fly' with Google Colab. The following function has to be modified accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvo3Ojs40MRL"
   },
   "source": [
    "#### Hints\n",
    "\n",
    "The provided function `process_frame()` detects and encodes all the faces in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnFPofAs0MRM"
   },
   "source": [
    "The provided function `process_movie()` detects and encodes the faces in the input video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUt43M9a0MRN"
   },
   "source": [
    "The special input `0` can be used to access the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O68ckoUO0MRL"
   },
   "outputs": [],
   "source": [
    "def process_frame(image, mode=\"fast\", model=None):\n",
    "\n",
    "    # face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if mode == \"fast\":\n",
    "        matches = hog_detector(gray,1)\n",
    "    else:\n",
    "        matches = cnn_detector(gray,1)\n",
    "        matches = [m.rect for m in matches]\n",
    "\n",
    "    for rect in matches:\n",
    "\n",
    "        # face classification\n",
    "        if model is None:\n",
    "            label = \"label\"\n",
    "        else:\n",
    "            # face landmarks\n",
    "            landmarks = pose68(gray, rect)\n",
    "             # face encoding\n",
    "            encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n",
    "            # Convert the encoding to the correct shape for prediction\n",
    "            encoding = np.array(encoding).reshape(1, -1)\n",
    "            # Predict the label\n",
    "            prediction = model.predict(encoding)\n",
    "            label = prediction[0]\n",
    "        \n",
    "        # draw box\n",
    "        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n",
    "        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n",
    "        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "def process_movie(video_name, outvideo_name='/kaggle/working/videoResult.mp4', mode=\"fast\", model=None):\n",
    "\n",
    "    video  = cv2.VideoCapture(video_name)\n",
    "    if (video.isOpened()== False): \n",
    "        print(\"Error opening video stream or file\")\n",
    "        return\n",
    "    \n",
    "    frame_width = int(video.get(3))\n",
    "    frame_height = int(video.get(4))\n",
    "    out_mp4 = cv2.VideoWriter(outvideo_name,cv2.VideoWriter_fourcc(*'XVID'), 10, (frame_width//2,frame_height//2))\n",
    "\n",
    "    i=0\n",
    "    while video.isOpened():\n",
    "\n",
    "        # Grab a single frame of video\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            # Resize frame of video for faster processing\n",
    "            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n",
    "            # Process frame\n",
    "            image = process_frame(frame, mode, model)\n",
    "            # Write the processed frame to output video\n",
    "            out_mp4.write(image)\n",
    "        else:\n",
    "            break\n",
    "        i += 1\n",
    "        if i==1000:\n",
    "            break\n",
    "    # Release video\n",
    "    video.release()\n",
    "    out_mp4.release()\n",
    "    print(\"Video released\")\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play(filename):\n",
    "    html = ''\n",
    "    video = open(filename,'rb').read()\n",
    "    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n",
    "    html += '<video width=1000 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n",
    "    return HTML(html)\n",
    "\n",
    "\n",
    "def suppr_test():\n",
    "    !rm test.zip\n",
    "    !rm test -r\n",
    "    !rm __MACOSX -r\n",
    "\n",
    "suppr_test()\n",
    "\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/test.zip\n",
    "!unzip -q test.zip\n",
    "!ls test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE PARTIE 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(train_data, train_labels)\n",
    "\n",
    "# SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(train_data, train_labels)\n",
    "\n",
    "# kNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=15)\n",
    "knn_model.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÉVALUATION DES MODÈLES\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, train_data, train_labels, test_data, test_labels):\n",
    "    start_time = time.time()\n",
    "    predictions = model.predict(test_data)\n",
    "    prediction_time = time.time() - start_time\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    return accuracy, prediction_time\n",
    "\n",
    "def evaluate_keras_model(model, train_data, train_labels, test_data, test_labels):\n",
    "    start_time = time.time()\n",
    "    loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n",
    "    prediction_time = time.time() - start_time\n",
    "    return accuracy, prediction_time\n",
    "\n",
    "logistic_accuracy, logistic_pred_time = evaluate_model(logistic_model, train_data, train_labels, test_data, test_labels)\n",
    "print(f'Logistic Regression - Accuracy: {logistic_accuracy:.2f}, Prediction Time: {logistic_pred_time:.4f}s')\n",
    "\n",
    "svm_accuracy, svm_pred_time = evaluate_model(svm_model, train_data, train_labels, test_data, test_labels)\n",
    "print(f'SVM - Accuracy: {svm_accuracy:.2f}, Prediction Time: {svm_pred_time:.4f}s')\n",
    "\n",
    "knn_accuracy, knn_pred_time = evaluate_model(knn_model, train_data, train_labels, test_data, test_labels)\n",
    "print(f'kNN - Accuracy: {knn_accuracy:.2f}, Prediction Time: {knn_pred_time:.4f}s')\n",
    "\n",
    "nn_accuracy, nn_pred_time = evaluate_keras_model(model, train_data, train_labels_onehot, test_data, test_labels_onehot)\n",
    "print(f'Neural Network - Accuracy: {nn_accuracy:.2f}, Prediction Time: {nn_pred_time:.4f}s')\n",
    "# model : réseau entraîné dans la partie précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur une des images fournie\n",
    "image = cv2.imread(\"test/example_03.png\")\n",
    "processed = process_frame(image.copy(), model=logistic_model)\n",
    "processed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur la vidéo fournie\n",
    "!rm videoResult.mp4\n",
    "!rm videoResult_fixed.mp4\n",
    "process_movie(\"test/lunch_scene.mp4\", mode=\"not fast\", model=logistic_model)\n",
    "# Reconvertir la vidéo avec ffmpeg en .mp4 compatible\n",
    "!ffmpeg -i videoResult.mp4 -vcodec libx264 -acodec aac videoResult_fixed.mp4 >/dev/null 2>&1\n",
    "play('/kaggle/working/videoResult_fixed.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aothdVBl0MRO"
   },
   "source": [
    "## 5. Build a custom dataset\n",
    "So far, you have used a pre-curated dataset, where somebody did the hard work of gathering and labeling the images for you. Now, you will tackle the problem of recognizing faces of yourselves, friends, family members, colleagues, etc. To accomplish this, you need to gather examples of faces you want to recognize. You can enroll facial pictures via a webcam attached to your computer.\n",
    "\n",
    "### Assignment\n",
    "Here's what you are required to do for this part of the assignment.\n",
    "\n",
    "- Use your webcam to enroll face pictures of yourself, your friends, etc. To do so, you need to open the webcam, detect faces in the video stream, and save the captured face images to disk.\n",
    "\n",
    "\n",
    "- Build a dataset of reasonable size: a group of 10-15 people with 50-100 face pictures each, taken in different conditions of light, angle, emotion, etc.\n",
    "\n",
    "\n",
    "- Apply the previously developed pipeline to build your own personalized face recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette partie peut être exécutée indépendamment des parties pécédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import cv2\n",
    "import dlib\n",
    "import os\n",
    "import keras\n",
    "import sklearn\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppr_models():\n",
    "    !rm models.zip\n",
    "    !rm models -r\n",
    "\n",
    "def suppr_figures():\n",
    "    !rm figures.zip\n",
    "    !rm figures -r\n",
    "\n",
    "def suppr_data():\n",
    "    !rm data.zip\n",
    "    !rm data -r\n",
    "    !rm __MACOSX -r\n",
    "\n",
    "suppr_models()\n",
    "suppr_figures()\n",
    "suppr_data()\n",
    "\n",
    "# importation des modèles préentraînés fournis\n",
    "!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n",
    "!unzip -q models.zip\n",
    "\n",
    "\n",
    "hog_detector = dlib.get_frontal_face_detector()\n",
    "cnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat') # nécessite un GPU\n",
    "\n",
    "# fonctions utilitaires\n",
    "def face_locations(image, model=\"hog\"):\n",
    "\n",
    "    if model == \"hog\":\n",
    "        detector = hog_detector\n",
    "        cst = 0\n",
    "    elif model == \"cnn\":\n",
    "        detector = cnn_detector\n",
    "        cst = 10\n",
    "\n",
    "    matches = detector(image,1)\n",
    "    rects   = []\n",
    "\n",
    "    for r in matches:\n",
    "        if model == \"cnn\":\n",
    "            r = r.rect\n",
    "        x = max(r.left(), 0)\n",
    "        y = max(r.top(), 0)\n",
    "        w = min(r.right(), image.shape[1]) - x + cst\n",
    "        h = min(r.bottom(), image.shape[0]) - y + cst\n",
    "        rects.append((x,y,w,h))\n",
    "\n",
    "    return rects\n",
    "\n",
    "def extract_faces(image, model=\"hog\"):\n",
    "\n",
    "    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = face_locations(gray, model)\n",
    "    faces = []\n",
    "\n",
    "    for (x,y,w,h) in rects:\n",
    "        cropped = image[y:y+h, x:x+w, :]\n",
    "        cropped = cv2.resize(cropped, (128,128))\n",
    "        faces.append(cropped)\n",
    "\n",
    "    return faces\n",
    "\n",
    "def show_grid(faces, figsize=(12,3)):\n",
    "\n",
    "    n = len(faces)\n",
    "    cols = 7\n",
    "    rows = int(np.ceil(n/cols))\n",
    "\n",
    "    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            i = r*cols + c\n",
    "            if i == n:\n",
    "                 break\n",
    "            ax[r,c].imshow(faces[i])\n",
    "            ax[r,c].axis('off')\n",
    "            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))\n",
    "\n",
    "def list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n",
    "\n",
    "    imagePaths = []\n",
    "\n",
    "    # loop over the directory structure\n",
    "    for (rootDir, dirNames, filenames) in os.walk(basePath):\n",
    "        # loop over the filenames in the current directory\n",
    "        for filename in filenames:\n",
    "            # if the contains string is not none and the filename does not contain\n",
    "            # the supplied string, then ignore the file\n",
    "            if contains is not None and filename.find(contains) == -1:\n",
    "                continue\n",
    "\n",
    "            # determine the file extension of the current file\n",
    "            ext = filename[filename.rfind(\".\"):].lower()\n",
    "\n",
    "            # check to see if the file is an image and should be processed\n",
    "            if ext.endswith(validExts):\n",
    "                # construct the path to the image and yield it\n",
    "                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n",
    "                imagePaths.append(imagePath)\n",
    "\n",
    "    return imagePaths\n",
    "\n",
    "pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\n",
    "pose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n",
    "\n",
    "def face_landmarks(face, model=\"large\"):\n",
    "\n",
    "    if model == \"large\":\n",
    "        predictor = pose68\n",
    "    elif model == \"small\":\n",
    "        predictor = pose05\n",
    "\n",
    "    if not isinstance(face, list):\n",
    "        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n",
    "        return predictor(face, rect)\n",
    "    else:\n",
    "        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n",
    "        return [predictor(f,rect) for f in face]\n",
    "\n",
    "def shape_to_coords(shape):\n",
    "    return np.float32([[p.x, p.y] for p in shape.parts()])\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "INNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\n",
    "OUTER_EYES_AND_NOSE = np.array([36, 45, 33])\n",
    "\n",
    "\n",
    "def align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n",
    "    faces = []\n",
    "    for (img, marks) in zip(images, landmarks):\n",
    "        imgDim = img.shape[0]\n",
    "        coords = shape_to_coords(marks)\n",
    "        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n",
    "        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n",
    "        faces.append(warped)\n",
    "    return faces\n",
    "\n",
    "\n",
    "base_dir = \"data\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Au choix :\n",
    "\n",
    "# importation du jeu de données brut\n",
    "!pip install -q gdown\n",
    "import gdown\n",
    "\n",
    "file_id = \"1I6GZo2uU3d6r51pzbfmJlE7LhQ-R2GKr\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "gdown.download(url, output=\"data.zip\", quiet=False)\n",
    "\n",
    "!unzip -q data.zip\n",
    "\n",
    "# Extraction des visages dans les mêmes répertoires data/prenom_nom\n",
    "\n",
    "imagePaths = list_images(\"data\")\n",
    "for imagePath in imagePaths:\n",
    "    if \"face\" in os.path.basename(imagePath): \n",
    "        continue\n",
    "    image = cv2.imread(imagePath)\n",
    "    if image is None:\n",
    "        print(f\"Erreur : Impossible de lire l'image {imagePath}. Elle sera ignorée.\")\n",
    "        continue\n",
    "    faces = extract_faces(image, model=\"cnn\")\n",
    "    if len(faces) == 0:\n",
    "        print(f\"Erreur : Aucun visage détecté dans l'image {imagePath}. Elle sera ignorée.\")\n",
    "        continue\n",
    "    if len(faces) > 1:\n",
    "        print(f\"Erreur : Plus d'un visage détecté dans l'image {imagePath}. Elle sera ignorée.\")\n",
    "        continue\n",
    "    face = faces[0]\n",
    "    face_filename = os.path.splitext(imagePath)[0] + \"_face.jpg\"\n",
    "    cv2.imwrite(face_filename, face)\n",
    "print(\"Extraction des visages terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ou bien :\n",
    "# Charger les visages déjà extraits (pour gagner du temps)\n",
    "suppr_data()\n",
    "\n",
    "!pip install -q gdown\n",
    "import gdown\n",
    "file_id = \"1OXezr6FgQIitVeKSzHaqtr4WUIQfOwIM\"\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "gdown.download(url, output=\"data_faces.zip\", quiet=False)\n",
    "!unzip -q data_faces.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des visages dans des répertoires d'entraînement, de validation et de test\n",
    "\n",
    "# Création des répertoires\n",
    "for path in [train_dir, val_dir, test_dir]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path) # suppression des répertoires déjà existants\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "# Récupération des répertoires prenom_nom dans data\n",
    "person_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))\n",
    "               and d not in [\"train\", \"validation\", \"test\"]]\n",
    "\n",
    "for person in person_dirs:\n",
    "    person_path = os.path.join(base_dir, person)\n",
    "\n",
    "    # Utilisation de list_images pour récupérer les images qui terminent par \"face.jpg\"\n",
    "    images = list_images(person_path, contains=\"_face.jpg\")\n",
    "\n",
    "    # Mélange aléatoire des images\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calcul des indices de découpage pour répartir les images en trois ensembles\n",
    "    total = len(images)\n",
    "    train_count = int(total * 0.7)  # % des images dans les données d'entraînement\n",
    "    val_count = int(total * 0.15)  # % des images pour la validation\n",
    "\n",
    "    # Découpage de l'ensemble des images en trois groupes : entraînement, validation et test\n",
    "    train_images = images[:train_count]\n",
    "    val_images = images[train_count:train_count + val_count]\n",
    "    test_images = images[train_count + val_count:]\n",
    "\n",
    "    # Définition des chemins de destination pour chaque ensemble\n",
    "    person_train_dir = os.path.join(train_dir, person)\n",
    "    person_val_dir = os.path.join(val_dir, person)\n",
    "    person_test_dir = os.path.join(test_dir, person)\n",
    "\n",
    "    # Création des répertoires de destination s'ils n'existent pas déjà\n",
    "    for path in [person_train_dir, person_val_dir, person_test_dir]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    # Déplacement des images dans les bons répertoires\n",
    "    for img in train_images:\n",
    "        shutil.move(img, os.path.join(person_train_dir, os.path.basename(img)))\n",
    "    for img in val_images:\n",
    "        shutil.move(img, os.path.join(person_val_dir, os.path.basename(img)))\n",
    "    for img in test_images:\n",
    "        shutil.move(img, os.path.join(person_test_dir, os.path.basename(img)))\n",
    "\n",
    "print(\"Répartition des visages dans les répertoires train, validation et test terminée.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2vUz-z20MRO"
   },
   "source": [
    "---\n",
    "## Credits\n",
    "\n",
    "This assignment is based on Adam Geitgey's [post](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
