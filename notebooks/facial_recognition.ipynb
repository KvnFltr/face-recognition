{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"colab":{"provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment B.5 - Modern face recognition with deep learning\n\nHave you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic. This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing: these algorithms can recognize faces with 98% accuracy, which is pretty much as good as humans can do!\n\nAs a human, your brain is wired to recognize faces automatically and instantly. Computers are not capable of doing this, so you have to teach them how to tackle each step in this process. Specifically, a face recognition system goes through four steps: find faces in the image, analyze their facial features, compare against known faces, and make a prediction of the corresponding persons. Here's described the full pipeline.\n\n<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/summary.gif\" style=\"height:200px;\">\n\n### Table of contents\n\nIn this assignment, you will tackle several problems related to face recognition:\n1. **Face detection**. Look at a picture and find all the faces in it. -5\n- **Pose estimation**. Understand where the face is turned and correct its pose. 5\n- **Face encoding**. Pick up unique features from a face that can be used to distinguish it from others. 5\n- **Face recognition**. Compare the unique features of a face to those of all the people in a database. 5\n- **Personal dataset**. Build a custom face recognition dataset. 2\n\nBy the end of this notebook, you will have your own face recognition system.","metadata":{"id":"O56jp96I0MQo"}},{"cell_type":"markdown","source":"### Required packages\n\nHere are the packages you will need during the assignment.\n- [Numpy](http://www.numpy.org)\n- [Keras](https://keras.io)\n- [OpenCV](https://opencv.org)\n- [Dlib](http://dlib.net).\n\n**Note**: In Anaconda Navigator, the package `dlib` can be installed from the **conda-forge** channel. In Google Colab, everything is readily available","metadata":{"id":"MxS9du380MQw"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython import display\nimport cv2\nimport dlib\nimport os\nimport keras\nimport sklearn\nimport random\nimport shutil\n\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers","metadata":{"id":"cy3onABl0MQ0","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:46:55.528437Z","iopub.execute_input":"2025-04-18T09:46:55.528844Z","iopub.status.idle":"2025-04-18T09:46:55.533980Z","shell.execute_reply.started":"2025-04-18T09:46:55.528790Z","shell.execute_reply":"2025-04-18T09:46:55.532928Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## 1. Face detection\n\nThe first step in your pipeline is face detection. Obviously you need to locate the faces in a photograph before you can try to tell them apart. If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action. Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But you’ll use it for a different purpose:  finding the areas of the image you want to pass on to the next step in your pipeline.\n\n<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/detection.jpg\" style=\"height:200px;\">","metadata":{"id":"EnCc8P7y0MQ1"}},{"cell_type":"markdown","source":"### Assignment\n\nHere's what you are required to do for this part of the assignment.\n\n- You are provided with a small dataset of pictures, where each picture contains exactly one face. Extract the faces and their labels (i.e., the person's names). Store them to a new file with the function `dump` in the package `pickle`.\n\n\n-  Normalize the cropped faces (i.e., divide the pixel values by 255), and split them in train set (70%) and test set (30%) with the function `train_test_split` in the package `sklearn`.\n\n\n- Train a small convnet and check its performance on the test set. Remember: don't use the test images for training.\n\n\n- Try to improve the performance of the baseline convnet by using all the tricks you have learned in the course.","metadata":{"id":"5A5esp2s0MQ2"}},{"cell_type":"markdown","source":"#### Provided functions\n\nHere you will find some useful functions to complete the assignment.","metadata":{"id":"nfqaWaKR0MQ3"}},{"cell_type":"code","source":"def suppr_models():\n    !rm models.zip\n    !rm models -r\n\ndef suppr_figures():\n    !rm figures.zip\n    !rm figures -r\n\ndef suppr_data():\n    !rm data.zip\n    !rm data -r\n    !rm __MACOSX -r\n\nsuppr_models()\nsuppr_figures()\nsuppr_data()\n\n!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n!unzip -q models.zip\n!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/figures.zip\n!unzip -q figures.zip\n!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/data.zip\n!unzip -q data.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:47:01.037419Z","iopub.execute_input":"2025-04-18T09:47:01.037816Z","iopub.status.idle":"2025-04-18T09:47:31.569592Z","shell.execute_reply.started":"2025-04-18T09:47:01.037766Z","shell.execute_reply":"2025-04-18T09:47:31.567915Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove 'figures.zip': No such file or directory\nrm: cannot remove 'figures': No such file or directory\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"hog_detector = dlib.get_frontal_face_detector()\ncnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat')\n\ndef face_locations(image, model=\"hog\"):\n\n    if model == \"hog\":\n        detector = hog_detector\n        cst = 0\n    elif model == \"cnn\":\n        detector = cnn_detector\n        cst = 10\n\n    matches = detector(image,1)\n    rects   = []\n\n    for r in matches:\n        if model == \"cnn\":\n            r = r.rect\n        x = max(r.left(), 0)\n        y = max(r.top(), 0)\n        w = min(r.right(), image.shape[1]) - x + cst\n        h = min(r.bottom(), image.shape[0]) - y + cst\n        rects.append((x,y,w,h))\n\n    return rects\n\ndef extract_faces(image, model=\"hog\"):\n\n    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    rects = face_locations(gray, model)\n    faces = []\n\n    for (x,y,w,h) in rects:\n        cropped = image[y:y+h, x:x+w, :]\n        cropped = cv2.resize(cropped, (128,128))\n        faces.append(cropped)\n\n    return faces\n\ndef show_grid(faces, figsize=(12,3)):\n\n    n = len(faces)\n    cols = 7\n    rows = int(np.ceil(n/cols))\n\n    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n\n    for r in range(rows):\n        for c in range(cols):\n            i = r*cols + c\n            if i == n:\n                 break\n            ax[r,c].imshow(faces[i])\n            ax[r,c].axis('off')\n            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))\n\ndef list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n\n    imagePaths = []\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n                imagePaths.append(imagePath)\n\n    return imagePaths\n\nbase_dir = \"data\"\ntrain_dir = os.path.join(base_dir, \"train\")\nval_dir = os.path.join(base_dir, \"validation\")\ntest_dir = os.path.join(base_dir, \"test\")","metadata":{"id":"0RW7kXCj0MQ4","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:53:57.331447Z","iopub.execute_input":"2025-04-18T09:53:57.331741Z","iopub.status.idle":"2025-04-18T09:53:57.644399Z","shell.execute_reply.started":"2025-04-18T09:53:57.331717Z","shell.execute_reply":"2025-04-18T09:53:57.643365Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"#### Hints\n\nThe provided function `extract_faces()` applies face detection to a single input image, and returns a list of 128x128 blocks containing the detected faces.","metadata":{"id":"vMuqxVrJ0MQ9"}},{"cell_type":"code","source":"image = cv2.imread(\"figures/faces.png\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(15,5))\nplt.imshow(image)\n\nfaces = extract_faces(image, \"cnn\")  # Replace 'cnn' with 'hog' for faster but less accurate results\nshow_grid(faces)","metadata":{"id":"AJtMFU3W0MQ-","executionInfo":{"status":"ok","timestamp":1612372015155,"user_tz":-60,"elapsed":1484,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"ea0f9d9a-8aa3-4bd2-f58e-5c4280d40365","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:59:16.937223Z","iopub.execute_input":"2025-04-14T10:59:16.937567Z","iopub.status.idle":"2025-04-14T10:59:18.022052Z","shell.execute_reply.started":"2025-04-14T10:59:16.937538Z","shell.execute_reply":"2025-04-14T10:59:18.021128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Moreover, the function `list_images()` locates all the jpeg/png/tiff files in a given folder (including its subfolders).","metadata":{"id":"rfp7kZK10MQ_"}},{"cell_type":"markdown","source":"## CODE PARTIE 1","metadata":{}},{"cell_type":"code","source":"# Observation des données\n\n!ls data/alan_grant/\n# Visualisation d'une image des données pour se faire une idée\nimpath = \"data/alan_grant/00000082.jpg\"\nif os.path.exists(impath):\n    image = cv2.imread(impath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:08:18.767114Z","iopub.execute_input":"2025-04-13T16:08:18.767459Z","iopub.status.idle":"2025-04-13T16:08:19.317136Z","shell.execute_reply.started":"2025-04-13T16:08:18.767435Z","shell.execute_reply":"2025-04-13T16:08:19.316044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Extraction des faces dans les mêmes répertoires data/prenom_nom\n\n# Liste tous les chemins d'images dans le répertoire \"data\"\nimagePaths = list_images(\"data\")\n\nfor imagePath in imagePaths:\n    # Si le nom de l'image contient le mot \"face\", on passe à l'image suivante\n    if \"face\" in os.path.basename(imagePath): \n        continue\n\n    # Lecture de l'image à partir du chemin\n    image = cv2.imread(imagePath)\n\n    # Extraction des visages de l'image\n    # Utilisation du modèle \"cnn\" pour une meilleure détection de face, bien que \"hog\" soit plus rapide \n    faces = extract_faces(image, model=\"cnn\")\n\n    # Si aucun visage n'est détecté, on passe à l'image suivante\n    if len(faces) == 0:\n        continue\n\n    # Comme il n'y a jamais plus d'un visage par image, on manipule directement le premier visage détectée\n    face = faces[0]\n\n    # Récupération du chemin complet en remplaçant l'extension (.png, .jpg...) par \"_face.jpg\"\n    face_filename = os.path.splitext(imagePath)[0] + \"_face.jpg\"\n\n    # Enregistrement du visage dans le même répertoire que l'image d'origine\n    cv2.imwrite(face_filename, face)\n\nprint(\"Extraction des visages terminée.\")\n\n\n# 2. Réarrangement des visages dans des répertoires d'entraînement, de validation et de test\n\n# Création des répertoires\nfor path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(path):\n        shutil.rmtree(path) # suppression des répertoires déjà existants\n    os.makedirs(path, exist_ok=True)\n\n# Récupération des répertoires prenom_nom dans data\nperson_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))\n               and d not in [\"train\", \"validation\", \"test\"]]\n\nfor person in person_dirs:\n    person_path = os.path.join(base_dir, person)\n\n    # Utilisation de list_images pour récupérer les images qui terminent par \"face.jpg\"\n    images = list_images(person_path, contains=\"_face.jpg\")\n\n    # Mélange aléatoire des images\n    random.shuffle(images)\n\n    # Calcul des indices de découpage pour répartir les images en trois ensembles\n    total = len(images)\n    train_count = int(total * 0.65)  # % des images dans les données d'entraînement\n    val_count = int(total * 0.2)  # % des images pour la validation\n\n    # Découpage de l'ensemble des images en trois groupes : entraînement, validation et test\n    train_images = images[:train_count]\n    val_images = images[train_count:train_count + val_count]\n    test_images = images[train_count + val_count:]\n\n    # Définition des chemins de destination pour chaque ensemble\n    person_train_dir = os.path.join(train_dir, person)\n    person_val_dir = os.path.join(val_dir, person)\n    person_test_dir = os.path.join(test_dir, person)\n\n    # Création des répertoires de destination s'ils n'existent pas déjà\n    for path in [person_train_dir, person_val_dir, person_test_dir]:\n        os.makedirs(path, exist_ok=True)\n\n    # Déplacement des images dans les bons répertoires\n    for img in train_images:\n        shutil.move(img, os.path.join(person_train_dir, os.path.basename(img)))\n    for img in val_images:\n        shutil.move(img, os.path.join(person_val_dir, os.path.basename(img)))\n    for img in test_images:\n        shutil.move(img, os.path.join(person_test_dir, os.path.basename(img)))\n\nprint(\"Répartition des visages dans les répertoires train, validation et test terminée.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:38:59.337230Z","iopub.execute_input":"2025-04-13T16:38:59.337605Z","iopub.status.idle":"2025-04-13T16:39:30.839726Z","shell.execute_reply.started":"2025-04-13T16:38:59.337577Z","shell.execute_reply":"2025-04-13T16:39:30.838664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Création des générateurs\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(128, 128),\n    batch_size=10,\n    class_mode='categorical' # categorical puisqu'on a 6 variables catégorielles (les différents personnages)\n)\n\nvalidation_generator = test_datagen.flow_from_directory(\n    val_dir,\n    target_size=(128, 128),\n    batch_size=10,          # Ajusté en f° du nombre d'images dans les données de validation\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(128, 128),\n    batch_size=10,\n    class_mode='categorical'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T20:02:17.479110Z","iopub.execute_input":"2025-04-13T20:02:17.479457Z","iopub.status.idle":"2025-04-13T20:02:17.562897Z","shell.execute_reply.started":"2025-04-13T20:02:17.479426Z","shell.execute_reply":"2025-04-13T20:02:17.562257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for data_batch, labels_batch in train_generator:\n\n    print('data batch shape:', data_batch.shape)\n    print('data label shape:', labels_batch.shape)\n\n    plt.imshow(data_batch[0])\n    plt.show()\n\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:27:39.018078Z","iopub.execute_input":"2025-04-13T16:27:39.018373Z","iopub.status.idle":"2025-04-13T16:27:39.244942Z","shell.execute_reply.started":"2025-04-13T16:27:39.018351Z","shell.execute_reply":"2025-04-13T16:27:39.244112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Création et entraînement du réseau de neurones\n\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(6, activation='softmax')) # softmax vu qu'on a 6 sorties\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['acc'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:40:05.712211Z","iopub.execute_input":"2025-04-13T16:40:05.712545Z","iopub.status.idle":"2025-04-13T16:40:05.804035Z","shell.execute_reply.started":"2025-04-13T16:40:05.712509Z","shell.execute_reply":"2025-04-13T16:40:05.803163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_generator,  # Use train_generator for training data\n    epochs=30,\n    validation_data=validation_generator  # Use validation_generator for validation data\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:40:07.234554Z","iopub.execute_input":"2025-04-13T16:40:07.234853Z","iopub.status.idle":"2025-04-13T16:40:17.907215Z","shell.execute_reply.started":"2025-04-13T16:40:07.234832Z","shell.execute_reply":"2025-04-13T16:40:17.906579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the training info\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\nacc      = history.history['acc']\nval_acc  = history.history['val_acc']\n\n# Visualize the history plots\nplt.figure()\nplt.plot(loss, 'b', label='Training loss')\nplt.plot(val_loss, 'm', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(acc, 'b', label='Training acc')\nplt.plot(val_acc, 'm', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:40:20.310107Z","iopub.execute_input":"2025-04-13T16:40:20.310414Z","iopub.status.idle":"2025-04-13T16:40:20.651615Z","shell.execute_reply.started":"2025-04-13T16:40:20.310391Z","shell.execute_reply":"2025-04-13T16:40:20.650891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss, val_acc = model.evaluate(validation_generator, steps=20)\nprint('Validation accuracy: {:2.2f}%'.format(val_acc*100))\ntest_loss, test_acc = model.evaluate(test_generator, steps=20)\nprint('Test accuracy: {:2.2f}%'.format(test_acc*100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:40:25.174615Z","iopub.execute_input":"2025-04-13T16:40:25.174910Z","iopub.status.idle":"2025-04-13T16:40:25.503390Z","shell.execute_reply.started":"2025-04-13T16:40:25.174890Z","shell.execute_reply":"2025-04-13T16:40:25.502552Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remarque : En premier lieu nous avons obtenu un résultat étonnant. Les performances du modèle sur l'ensemble de test sont systématiquement meilleures que sur l'ensemble de validation. Nous avons tenter de faire varier les % de répartition des images dans les différents ensemble, et changé le batch size. Finalement, il semblerait que le problème soit lié au fait que les images de test soient plus faciles que les images de validation. Nous avons donc ajouté un mélange aléatoire des images avant répartition pour résoudre le problème. Il n'est cependant pas garanti à 100% que les performances de test soient toujours inférieures aux performances de validation.","metadata":{}},{"cell_type":"markdown","source":"## 2. Pose estimation\n\nYou have isolated the faces in our image. But now you have to deal with the problem that faces turned different directions look totally different to a computer. To account for this, you will try to warp each picture so that the eyes and lips are always in the same place in the image. More concretely, you are going to use an algorithm called face landmark estimation. The basic idea is to locate 68 specific points (called landmarks) that exist on every face:  the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then, you’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. This will make face recognition more accurate.\n\n<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/pose.png\" style=\"height:200px;\">","metadata":{"id":"sFrGlU2q0MRB"}},{"cell_type":"markdown","source":"### Assignment\n\nHere's what you are required to do for this part of the assignment.\n\n- Further preprocess the face pictures by correcting their pose. You should now have a dataset of cropped, aligned, and normalized faces.\n\n\n- Re-train your convnets on the modified dataset.\n\n\n- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets.","metadata":{"id":"qLBTOO7F0MRB"}},{"cell_type":"markdown","source":"#### Provided functions\n\nHere you will find some useful functions to complete the assignment.","metadata":{"id":"xEeUbQSn0MRB"}},{"cell_type":"code","source":"pose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\npose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n\ndef face_landmarks(face, model=\"large\"):\n\n    if model == \"large\":\n        predictor = pose68\n    elif model == \"small\":\n        predictor = pose05\n\n    if not isinstance(face, list):\n        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n        return predictor(face, rect)\n    else:\n        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n        return [predictor(f,rect) for f in face]\n\ndef shape_to_coords(shape):\n    return np.float32([[p.x, p.y] for p in shape.parts()])\n\nTEMPLATE = np.float32([\n    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n\nTPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\nMINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n\nINNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\nOUTER_EYES_AND_NOSE = np.array([36, 45, 33])\n\n\ndef align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n    faces = []\n    for (img, marks) in zip(images, landmarks):\n        imgDim = img.shape[0]\n        coords = shape_to_coords(marks)\n        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n        faces.append(warped)\n    return faces","metadata":{"id":"BgIWJ13c0MRB","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:59:59.445368Z","iopub.execute_input":"2025-04-14T10:59:59.445661Z","iopub.status.idle":"2025-04-14T11:00:00.333091Z","shell.execute_reply.started":"2025-04-14T10:59:59.445640Z","shell.execute_reply":"2025-04-14T11:00:00.332203Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Hints\n\nThe provided function `face_landmarks()` computes the landmarks for a list of cropped faces.","metadata":{"id":"t5S7-ag80MRC"}},{"cell_type":"code","source":"landmarks = face_landmarks(faces)\n\nnew_faces = []\nfor (face,shape) in zip(faces, landmarks):\n    canvas = face.copy()\n    coords = shape_to_coords(shape)\n    for p in coords:\n        cv2.circle(canvas, (int(p[0]),int(p[1])), 1, (0, 0, 255), -1)\n    new_faces.append(canvas)\n\nshow_grid(new_faces, figsize=(15,5))\n\naligned = align_faces(faces, landmarks)\nshow_grid(aligned, figsize=(15,5))","metadata":{"id":"5jItizFr0MRE","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:58:01.689829Z","iopub.execute_input":"2025-04-13T16:58:01.690110Z","iopub.status.idle":"2025-04-13T16:58:02.729068Z","shell.execute_reply.started":"2025-04-13T16:58:01.690089Z","shell.execute_reply":"2025-04-13T16:58:02.728124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow( np.stack(aligned, axis=3).astype(np.float32).mean(axis=3)/255 )","metadata":{"id":"bfTSgga90MRH","executionInfo":{"status":"ok","timestamp":1612372120733,"user_tz":-60,"elapsed":849,"user":{"displayName":"Laurent NAJMAN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiIQq-pEPOb4GOLKkKealDH8DhzBENKDk5Nj45Z=s64","userId":"14324108496982386588"}},"outputId":"79aa80d2-b1b4-4fc6-be9b-26a3c7c0af57","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:57:51.777590Z","iopub.execute_input":"2025-04-13T16:57:51.777883Z","iopub.status.idle":"2025-04-13T16:57:52.000457Z","shell.execute_reply.started":"2025-04-13T16:57:51.777862Z","shell.execute_reply":"2025-04-13T16:57:51.999562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CODE PARTIE 2","metadata":{}},{"cell_type":"code","source":"# Conversion de chaque visages du répertoire data en visages centrés\n# Chaque visage est traité individuellement\ndef center_faces(base_dir):\n    # Lister toutes les images terminant par \"_face.jpg\"\n    imagePaths = list_images(base_dir, contains=\"_face.jpg\")\n\n    for imagePath in imagePaths:\n        # Lire l'image\n        image = cv2.imread(imagePath)\n\n        # Détecter les points de repère\n        landmarks = face_landmarks(image)\n\n        # Aligner le visage\n        aligned_face = align_faces([image], [landmarks])[0]\n\n        # Construire le chemin pour sauvegarder l'image centrée\n        newImagePath = imagePath.replace(\"_face.jpg\", \"_centered.jpg\")\n\n        # Sauvegarder l'image centrée\n        cv2.imwrite(newImagePath, aligned_face)\n\n        # Supprimer l'image originale (_face.jpg)\n        os.remove(imagePath)\n        \n    print(\"Traitement des images terminé.\")\n\ncenter_faces(base_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:00:05.982460Z","iopub.execute_input":"2025-04-14T11:00:05.982754Z","iopub.status.idle":"2025-04-14T11:00:05.990767Z","shell.execute_reply.started":"2025-04-14T11:00:05.982732Z","shell.execute_reply":"2025-04-14T11:00:05.989844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls data/train/alan_grant/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:58:47.184964Z","iopub.execute_input":"2025-04-13T16:58:47.185241Z","iopub.status.idle":"2025-04-13T16:58:47.372752Z","shell.execute_reply.started":"2025-04-13T16:58:47.185222Z","shell.execute_reply":"2025-04-13T16:58:47.371645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for data_batch, labels_batch in train_generator:\n\n    print('data batch shape:', data_batch.shape)\n    print('data label shape:', labels_batch.shape)\n\n    plt.imshow(data_batch[0])\n    plt.show()\n\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T16:59:47.384550Z","iopub.execute_input":"2025-04-13T16:59:47.384865Z","iopub.status.idle":"2025-04-13T16:59:47.583721Z","shell.execute_reply.started":"2025-04-13T16:59:47.384842Z","shell.execute_reply":"2025-04-13T16:59:47.582893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(6, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(learning_rate=1e-4), metrics=['acc'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:29.850856Z","iopub.execute_input":"2025-04-13T17:00:29.851171Z","iopub.status.idle":"2025-04-13T17:00:29.946230Z","shell.execute_reply.started":"2025-04-13T17:00:29.851148Z","shell.execute_reply":"2025-04-13T17:00:29.945629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_generator,  # Use train_generator for training data\n    epochs=30,\n    validation_data=validation_generator  # Use validation_generator for validation data\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:37.955823Z","iopub.execute_input":"2025-04-13T17:00:37.956104Z","iopub.status.idle":"2025-04-13T17:00:48.504556Z","shell.execute_reply.started":"2025-04-13T17:00:37.956082Z","shell.execute_reply":"2025-04-13T17:00:48.503884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the training info\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\nacc      = history.history['acc']\nval_acc  = history.history['val_acc']\n\n# Visualize the history plots\nplt.figure()\nplt.plot(loss, 'b', label='Training loss')\nplt.plot(val_loss, 'm', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(acc, 'b', label='Training acc')\nplt.plot(val_acc, 'm', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:00:48.505791Z","iopub.execute_input":"2025-04-13T17:00:48.506104Z","iopub.status.idle":"2025-04-13T17:00:48.880530Z","shell.execute_reply.started":"2025-04-13T17:00:48.506073Z","shell.execute_reply":"2025-04-13T17:00:48.879702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_loss, validation_accuracy = model.evaluate(validation_generator)\nprint('Validation accuracy: {:2.2f}%'.format(validation_accuracy * 100))\n\ntest_loss, test_acc = model.evaluate(test_generator)\nprint('Test accuracy: {:2.2f}%'.format(test_acc*100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T17:01:04.789973Z","iopub.execute_input":"2025-04-13T17:01:04.790295Z","iopub.status.idle":"2025-04-13T17:01:05.134076Z","shell.execute_reply.started":"2025-04-13T17:01:04.790270Z","shell.execute_reply":"2025-04-13T17:01:05.133311Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Face encoding\n\nThe simplest approach to face recognition is to directly classify an unknown face with a convnet trained on your database of tagged people. Seems like a pretty good idea, right? There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly train such a big convnet. That would take way too long. What you need is a way to extract a few basic measurements from each face, which you can then use to quickly compare the unknown face with your database. For example, you might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. However, it turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n\nThe solution is to train a convnet. But instead of training the network to classify pictures, it is trained to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: the picture of a known person, another picture of the same known person, and a picture of a totally different person. Then, the algorithm looks at the measurements currently generated for each of those three images. It tweaks the neural network slightly to make sure that the measurements generated for the same person are slightly closer, and the measurements for different persons are slightly further apart. After repeating this step millions of times for millions of images of thousands of different people, the convnet learns to generate 128 measurements for each person.\n\n<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/triplet.png\" style=\"height:400px;\">\n\nThis process of training a convnet to output face encodings requires a lot of data and computer power. Even with an expensive GPU, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Fortunately, the people at [OpenFace](https://cmusatyalab.github.io/openface/) already did this and they published several trained networks which you can directly use. So all you need to do is run your face images through their pre-trained network to get the 128 measurements for each face.\n\n<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/encoding.png\" style=\"height:300px;\">","metadata":{"id":"MCQgD33c0MRH"}},{"cell_type":"markdown","source":"### Assignment\n\nHere's what you are required to do for this part of the assignment.\n\n- Preprocess the cropped faces by encoding them. You should now have a dataset of cropped and encoded faces.\n\n\n- Train a neural network on the modified dataset. Since the encoded faces are just 128-length vectors, **you don't need a convnet**. Use a regular neural network with a series of fully-connected layers.\n\n\n- Evaluate the performance on the test set, and compare it to the scores obtained with your previously trained convnets.","metadata":{"id":"wo7Hduwu0MRH"}},{"cell_type":"markdown","source":"#### Provided functions","metadata":{"id":"KWu06RRG0MRH"}},{"cell_type":"code","source":"cnn_encoder = dlib.face_recognition_model_v1('models/dlib_face_recognition_resnet_model_v1.dat')\n\ndef face_encoder(faces):\n\n    landmarks = face_landmarks(faces)\n\n    if not isinstance(faces, list):\n        return np.array(cnn_encoder.compute_face_descriptor(faces,landmarks))\n    else:\n        return np.array([cnn_encoder.compute_face_descriptor(f,l) for f,l in zip(faces,landmarks)])\n\n\nencoded_faces = face_encoder(faces)\n\nplt.plot(encoded_faces[0])","metadata":{"id":"deJ2n5b00MRI","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:00:12.759153Z","iopub.execute_input":"2025-04-14T11:00:12.759488Z","iopub.status.idle":"2025-04-14T11:00:13.093953Z","shell.execute_reply.started":"2025-04-14T11:00:12.759458Z","shell.execute_reply":"2025-04-14T11:00:13.093107Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Hints\n\nThe provided function `face_encoder()` computes the encodings for a list of cropped faces. Alignment and normalization are handled internally.","metadata":{"id":"72ujJWBu0MRJ"}},{"cell_type":"markdown","source":"## CODE PARTIE 3","metadata":{}},{"cell_type":"code","source":"def encode_faces(directory):\n    data = []\n    labels = []\n\n    # Lister tous les sous-répertoires dans le répertoire donné\n    for person_dir in os.listdir(directory):\n        person_path = os.path.join(directory, person_dir)\n\n        # Lister toutes les images terminant par \"_centered.jpg\"\n        image_paths = [os.path.join(person_path, file_name) for file_name in os.listdir(person_path) if file_name.endswith('_centered.jpg')]\n        # Lire toutes les images\n        images = [cv2.imread(image_path) for image_path in image_paths]\n\n        # Encoder les visages\n        encoded_faces = face_encoder(images)\n\n        # Ajouter les vecteurs encodés et les labels aux listes\n        data.extend(encoded_faces)\n        labels.extend([person_dir] * len(encoded_faces))\n\n\n    return np.array(data), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:00:15.145858Z","iopub.execute_input":"2025-04-14T11:00:15.146151Z","iopub.status.idle":"2025-04-14T11:00:15.151537Z","shell.execute_reply.started":"2025-04-14T11:00:15.146127Z","shell.execute_reply":"2025-04-14T11:00:15.150520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n\ntrain_data, train_labels = encode_faces(train_dir)\nval_data, val_labels = encode_faces(val_dir)\ntest_data, test_labels = encode_faces(test_dir)\n\n# Encodage one-hot des labels\nlabel_binarizer = LabelBinarizer()\ntrain_labels_onehot = label_binarizer.fit_transform(train_labels)\nval_labels_onehot = label_binarizer.transform(val_labels)\ntest_labels_onehot = label_binarizer.transform(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:00:20.223902Z","iopub.execute_input":"2025-04-14T11:00:20.224225Z","iopub.status.idle":"2025-04-14T11:00:21.356812Z","shell.execute_reply.started":"2025-04-14T11:00:20.224198Z","shell.execute_reply":"2025-04-14T11:00:21.356123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# création du réseau\nmodel = models.Sequential()\nmodel.add(layers.Dense(128, activation='relu', input_shape=(128,)))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(train_labels_onehot.shape[1], activation='softmax')) # REMARK: softmax is for multi-class classification\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T10:44:59.841660Z","iopub.execute_input":"2025-04-14T10:44:59.841938Z","iopub.status.idle":"2025-04-14T10:45:01.269101Z","shell.execute_reply.started":"2025-04-14T10:44:59.841918Z","shell.execute_reply":"2025-04-14T10:45:01.268475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(train_data, train_labels_onehot, validation_data=(val_data, val_labels_onehot), epochs=40, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T20:02:48.549067Z","iopub.execute_input":"2025-04-13T20:02:48.549380Z","iopub.status.idle":"2025-04-13T20:02:54.002101Z","shell.execute_reply.started":"2025-04-13T20:02:48.549355Z","shell.execute_reply":"2025-04-13T20:02:54.001435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the training info\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\nacc      = history.history['accuracy']\nval_acc  = history.history['val_accuracy']\n\n# Visualize the history plots\nplt.figure()\nplt.plot(loss, 'b', label='Training loss')\nplt.plot(val_loss, 'm', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(acc, 'b', label='Training acc')\nplt.plot(val_acc, 'm', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T20:02:57.741030Z","iopub.execute_input":"2025-04-13T20:02:57.741369Z","iopub.status.idle":"2025-04-13T20:02:58.134073Z","shell.execute_reply.started":"2025-04-13T20:02:57.741340Z","shell.execute_reply":"2025-04-13T20:02:58.133246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Évaluer le modèle sur l'ensemble de validation\nval_loss, val_accuracy = model.evaluate(val_data, val_labels_onehot)\nprint('Validation accuracy: {:2.2f}%'.format(val_accuracy * 100))\n\n# Évaluer le modèle sur l'ensemble de test\ntest_loss, test_accuracy = model.evaluate(test_data, test_labels_onehot)\nprint('Test accuracy: {:2.2f}%'.format(test_accuracy * 100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T20:03:11.799071Z","iopub.execute_input":"2025-04-13T20:03:11.799426Z","iopub.status.idle":"2025-04-13T20:03:12.132032Z","shell.execute_reply.started":"2025-04-13T20:03:11.799398Z","shell.execute_reply":"2025-04-13T20:03:12.131153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Face recognition\n\nThis last step is actually the easiest one in the whole process. All you have to do is find the person in your database of known people who has the closest measurements to some test image. You can do that by using any machine learning classification algorithm, such as neaural network (as you did in the previous section), logistic regression, SVM, nearest neighbours, etc. All you need to do is training a classifier that can take in the measurements from a new test image, and tells which known person is the closest match. Running this classifier must only take milliseconds, so that you can apply it to video sequences.\n\n<img src=\"https://perso.esiee.fr/~najmanl/FaceRecognition/figures/test.gif\" style=\"height:300px;\">","metadata":{"id":"b7x18Hp30MRK"}},{"cell_type":"markdown","source":"### Assignment\n\nHere's what you are required to do for this part of the assignment.\n\n-  Train several classifiers (logistic regression, SVM, kNN, neural network) on the dataset of encoded faces (you can use the package `scikit-learn`).\n\n- Evaluate their performance on the test set, in terms of accuracy and speed.\n\n- Finally, run your best classifier on the test images and video available in the `test` folder.\n","metadata":{"id":"c3IRpEzh0MRK"}},{"cell_type":"markdown","source":"#### Provided functions","metadata":{"id":"XYHj2ZUZ0MRK"}},{"cell_type":"markdown","source":"**Note:** cv2.VideoCapture does not work in Google Colab. You can use https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi to capture video 'on the fly' with Google Colab. The following function has to be modified accordingly","metadata":{"id":"cMPHcAJI4diD"}},{"cell_type":"markdown","source":"#### Hints\n\nThe provided function `process_frame()` detects and encodes all the faces in the input image.","metadata":{"id":"bvo3Ojs40MRL"}},{"cell_type":"markdown","source":"The provided function `process_movie()` detects and encodes the faces in the input video.","metadata":{"id":"rnFPofAs0MRM"}},{"cell_type":"markdown","source":"The special input `0` can be used to access the webcam.","metadata":{"id":"EUt43M9a0MRN"}},{"cell_type":"code","source":"def process_frame(image, mode=\"fast\", model=None):\n\n    # face detection\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    if mode == \"fast\":\n        matches = hog_detector(gray,1)\n    else:\n        matches = cnn_detector(gray,1)\n        matches = [m.rect for m in matches]\n\n    for rect in matches:\n\n        # face classification\n        if model is None:\n            label = \"label\"\n        else:\n            # face landmarks\n            landmarks = pose68(gray, rect)\n             # face encoding\n            encoding = cnn_encoder.compute_face_descriptor(image, landmarks)\n            # Convert the encoding to the correct shape for prediction\n            encoding = np.array(encoding).reshape(1, -1)\n            # Predict the label\n            prediction = model.predict(encoding)\n            label = prediction[0]\n        \n        # draw box\n        cv2.rectangle(image, (rect.left(), rect.top()), (rect.right(), rect.bottom()), (0, 255, 0), 2)\n        y = rect.top() - 15 if rect.top() - 15 > 15 else rect.bottom() + 25\n        cv2.putText(image, label, (rect.left(), y), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n\n    return image\n\ndef process_movie(video_name, outvideo_name='/kaggle/working/videoResult.mp4', mode=\"fast\", model=None):\n\n    video  = cv2.VideoCapture(video_name)\n    if (video.isOpened()== False): \n        print(\"Error opening video stream or file\")\n        return\n    \n    frame_width = int(video.get(3))\n    frame_height = int(video.get(4))\n    out_mp4 = cv2.VideoWriter(outvideo_name,cv2.VideoWriter_fourcc(*'XVID'), 10, (frame_width//2,frame_height//2))\n\n    i=0\n    while video.isOpened():\n\n        # Grab a single frame of video\n        ret, frame = video.read()\n        if ret == True:\n            # Resize frame of video for faster processing\n            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n            # Process frame\n            image = process_frame(frame, mode, model)\n            # Write the processed frame to output video\n            out_mp4.write(image)\n        else:\n            break\n        i += 1\n        if i==1000:\n            break\n    # Release video\n    video.release()\n    out_mp4.release()\n    print(\"Video released\")\n\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\ndef play(filename):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=1000 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n    return HTML(html)\n\n\ndef suppr_test():\n    !rm test.zip\n    !rm test -r\n    !rm __MACOSX -r\n\nsuppr_test()\n\n!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/test.zip\n!unzip -q test.zip\n!ls test","metadata":{"id":"O68ckoUO0MRL","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:04:42.329320Z","iopub.execute_input":"2025-04-14T11:04:42.329667Z","iopub.status.idle":"2025-04-14T11:04:45.259628Z","shell.execute_reply.started":"2025-04-14T11:04:42.329643Z","shell.execute_reply":"2025-04-14T11:04:45.258453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CODE PARTIE 4","metadata":{}},{"cell_type":"code","source":"# Logistic regression classifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogistic_model = LogisticRegression(max_iter=1000)\nlogistic_model.fit(train_data, train_labels)\n\n# SVM classifier\nfrom sklearn.svm import SVC\n\nsvm_model = SVC(kernel='linear', probability=True)\nsvm_model.fit(train_data, train_labels)\n\n# kNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors=15)\nknn_model.fit(train_data, train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:01:11.818808Z","iopub.execute_input":"2025-04-14T11:01:11.819097Z","iopub.status.idle":"2025-04-14T11:01:11.856488Z","shell.execute_reply.started":"2025-04-14T11:01:11.819076Z","shell.execute_reply":"2025-04-14T11:01:11.855839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ÉVALUATION DES MODÈLES\nimport time\nfrom sklearn.metrics import accuracy_score\n\ndef evaluate_model(model, train_data, train_labels, test_data, test_labels):\n    start_time = time.time()\n    predictions = model.predict(test_data)\n    prediction_time = time.time() - start_time\n    accuracy = accuracy_score(test_labels, predictions)\n    return accuracy, prediction_time\n\ndef evaluate_keras_model(model, train_data, train_labels, test_data, test_labels):\n    start_time = time.time()\n    loss, accuracy = model.evaluate(test_data, test_labels, verbose=0)\n    prediction_time = time.time() - start_time\n    return accuracy, prediction_time\n\nlogistic_accuracy, logistic_pred_time = evaluate_model(logistic_model, train_data, train_labels, test_data, test_labels)\nprint(f'Logistic Regression - Accuracy: {logistic_accuracy:.2f}, Prediction Time: {logistic_pred_time:.4f}s')\n\nsvm_accuracy, svm_pred_time = evaluate_model(svm_model, train_data, train_labels, test_data, test_labels)\nprint(f'SVM - Accuracy: {svm_accuracy:.2f}, Prediction Time: {svm_pred_time:.4f}s')\n\nknn_accuracy, knn_pred_time = evaluate_model(knn_model, train_data, train_labels, test_data, test_labels)\nprint(f'kNN - Accuracy: {knn_accuracy:.2f}, Prediction Time: {knn_pred_time:.4f}s')\n\nnn_accuracy, nn_pred_time = evaluate_keras_model(model, train_data, train_labels_onehot, test_data, test_labels_onehot)\nprint(f'Neural Network - Accuracy: {nn_accuracy:.2f}, Prediction Time: {nn_pred_time:.4f}s')\n# model : réseau entraîné dans la partie précédente","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:01:17.208483Z","iopub.execute_input":"2025-04-14T11:01:17.208783Z","iopub.status.idle":"2025-04-14T11:01:17.283884Z","shell.execute_reply.started":"2025-04-14T11:01:17.208761Z","shell.execute_reply":"2025-04-14T11:01:17.283081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test sur une des images fournie\nimage = cv2.imread(\"test/example_03.png\")\nprocessed = process_frame(image.copy(), model=logistic_model)\nprocessed = cv2.cvtColor(processed, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(15,5))\nplt.imshow(processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:03:25.791820Z","iopub.execute_input":"2025-04-14T11:03:25.792117Z","iopub.status.idle":"2025-04-14T11:03:26.255142Z","shell.execute_reply.started":"2025-04-14T11:03:25.792095Z","shell.execute_reply":"2025-04-14T11:03:26.254150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test sur la vidéo fournie\n!rm videoResult.mp4\n!rm videoResult_fixed.mp4\nprocess_movie(\"test/lunch_scene.mp4\", mode=\"not fast\", model=logistic_model)\n# Reconvertir la vidéo avec ffmpeg en .mp4 compatible\n!ffmpeg -i videoResult.mp4 -vcodec libx264 -acodec aac videoResult_fixed.mp4 >/dev/null 2>&1\nplay('/kaggle/working/videoResult_fixed.mp4')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T11:07:08.549985Z","iopub.execute_input":"2025-04-14T11:07:08.550386Z","iopub.status.idle":"2025-04-14T11:07:27.967507Z","shell.execute_reply.started":"2025-04-14T11:07:08.550352Z","shell.execute_reply":"2025-04-14T11:07:27.966277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Build a custom dataset\nSo far, you have used a pre-curated dataset, where somebody did the hard work of gathering and labeling the images for you. Now, you will tackle the problem of recognizing faces of yourselves, friends, family members, colleagues, etc. To accomplish this, you need to gather examples of faces you want to recognize. You can enroll facial pictures via a webcam attached to your computer.\n\n### Assignment\nHere's what you are required to do for this part of the assignment.\n\n- Use your webcam to enroll face pictures of yourself, your friends, etc. To do so, you need to open the webcam, detect faces in the video stream, and save the captured face images to disk.\n\n\n- Build a dataset of reasonable size: a group of 10-15 people with 50-100 face pictures each, taken in different conditions of light, angle, emotion, etc.\n\n\n- Apply the previously developed pipeline to build your own personalized face recognition system.","metadata":{"id":"aothdVBl0MRO"}},{"cell_type":"code","source":"# Cette partie peut être exécutée indépendamment des parties pécédentes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython import display\nimport cv2\nimport dlib\nimport os\nimport keras\nimport sklearn\nimport random\nimport shutil\n\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T11:28:46.512049Z","iopub.execute_input":"2025-04-18T11:28:46.512308Z","iopub.status.idle":"2025-04-18T11:28:46.516171Z","shell.execute_reply.started":"2025-04-18T11:28:46.512286Z","shell.execute_reply":"2025-04-18T11:28:46.515420Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def suppr_models():\n    !rm models.zip\n    !rm models -r\n\ndef suppr_figures():\n    !rm figures.zip\n    !rm figures -r\n\ndef suppr_data():\n    !rm data.zip\n    !rm data -r\n    !rm __MACOSX -r\n\nsuppr_models()\nsuppr_figures()\nsuppr_data()\n\n# importation des modèles préentraînés fournis\n!wget -q https://perso.esiee.fr/~najmanl/FaceRecognition/models.zip\n!unzip -q models.zip\n\n\nhog_detector = dlib.get_frontal_face_detector()\ncnn_detector = dlib.cnn_face_detection_model_v1('models/mmod_human_face_detector.dat') # nécessite un GPU\n\n# fonctions utilitaires\ndef face_locations(image, model=\"hog\"):\n\n    if model == \"hog\":\n        detector = hog_detector\n        cst = 0\n    elif model == \"cnn\":\n        detector = cnn_detector\n        cst = 10\n\n    matches = detector(image,1)\n    rects   = []\n\n    for r in matches:\n        if model == \"cnn\":\n            r = r.rect\n        x = max(r.left(), 0)\n        y = max(r.top(), 0)\n        w = min(r.right(), image.shape[1]) - x + cst\n        h = min(r.bottom(), image.shape[0]) - y + cst\n        rects.append((x,y,w,h))\n\n    return rects\n\ndef extract_faces(image, model=\"hog\"):\n\n    gray  = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    rects = face_locations(gray, model)\n    faces = []\n\n    for (x,y,w,h) in rects:\n        cropped = image[y:y+h, x:x+w, :]\n        cropped = cv2.resize(cropped, (128,128))\n        faces.append(cropped)\n\n    return faces\n\ndef show_grid(faces, figsize=(12,3)):\n\n    n = len(faces)\n    cols = 7\n    rows = int(np.ceil(n/cols))\n\n    fig, ax = plt.subplots(rows,cols, figsize=figsize)\n\n    for r in range(rows):\n        for c in range(cols):\n            i = r*cols + c\n            if i == n:\n                 break\n            ax[r,c].imshow(faces[i])\n            ax[r,c].axis('off')\n            #ax[r,c].set_title('size: ' + str(faces[i].shape[:2]))\n\ndef list_images(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"), contains=None):\n\n    imagePaths = []\n\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n                imagePaths.append(imagePath)\n\n    return imagePaths\n\npose68 = dlib.shape_predictor('models/shape_predictor_68_face_landmarks.dat')\npose05 = dlib.shape_predictor('models/shape_predictor_5_face_landmarks.dat')\n\ndef face_landmarks(face, model=\"large\"):\n\n    if model == \"large\":\n        predictor = pose68\n    elif model == \"small\":\n        predictor = pose05\n\n    if not isinstance(face, list):\n        rect = dlib.rectangle(0,0,face.shape[1],face.shape[0])\n        return predictor(face, rect)\n    else:\n        rect = dlib.rectangle(0,0,face[0].shape[1],face[0].shape[0])\n        return [predictor(f,rect) for f in face]\n\ndef shape_to_coords(shape):\n    return np.float32([[p.x, p.y] for p in shape.parts()])\n\nTEMPLATE = np.float32([\n    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n\nTPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\nMINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n\nINNER_EYES_AND_BOTTOM_LIP = np.array([39, 42, 57])\nOUTER_EYES_AND_NOSE = np.array([36, 45, 33])\n\n\ndef align_faces(images, landmarks, idx=INNER_EYES_AND_BOTTOM_LIP):\n    faces = []\n    for (img, marks) in zip(images, landmarks):\n        imgDim = img.shape[0]\n        coords = shape_to_coords(marks)\n        H = cv2.getAffineTransform(coords[idx], imgDim * MINMAX_TEMPLATE[idx])\n        warped = cv2.warpAffine(img, H, (imgDim, imgDim))\n        faces.append(warped)\n    return faces\n\n\nbase_dir = \"data\"\ntrain_dir = os.path.join(base_dir, \"train\")\nval_dir = os.path.join(base_dir, \"validation\")\ntest_dir = os.path.join(base_dir, \"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T11:29:04.363035Z","iopub.execute_input":"2025-04-18T11:29:04.363319Z","iopub.status.idle":"2025-04-18T11:29:33.584586Z","shell.execute_reply.started":"2025-04-18T11:29:04.363298Z","shell.execute_reply":"2025-04-18T11:29:33.583698Z"}},"outputs":[{"name":"stdout","text":"rm: cannot remove 'figures.zip': No such file or directory\nrm: cannot remove 'figures': No such file or directory\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1I6GZo2uU3d6r51pzbfmJlE7LhQ-R2GKr\nFrom (redirected): https://drive.google.com/uc?id=1I6GZo2uU3d6r51pzbfmJlE7LhQ-R2GKr&confirm=t&uuid=2ec1ec41-b799-42a9-8a28-bdf5071102d8\nTo: /kaggle/working/data.zip\n100%|██████████| 208M/208M [00:07<00:00, 27.6MB/s] \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Au choix :\n\n# importation du jeu de données brut\n!pip install -q gdown\nimport gdown\n\nfile_id = \"1I6GZo2uU3d6r51pzbfmJlE7LhQ-R2GKr\"\nurl = f\"https://drive.google.com/uc?id={file_id}\"\ngdown.download(url, output=\"data.zip\", quiet=False)\n\n!unzip -q data.zip\n\n# Extraction des visages dans les mêmes répertoires data/prenom_nom\n\nimagePaths = list_images(\"data\")\nfor imagePath in imagePaths:\n    if \"face\" in os.path.basename(imagePath): \n        continue\n    image = cv2.imread(imagePath)\n    if image is None:\n        print(f\"Erreur : Impossible de lire l'image {imagePath}. Elle sera ignorée.\")\n        continue\n    faces = extract_faces(image, model=\"cnn\")\n    if len(faces) == 0:\n        print(f\"Erreur : Aucun visage détecté dans l'image {imagePath}. Elle sera ignorée.\")\n        continue\n    if len(faces) > 1:\n        print(f\"Erreur : Plus d'un visage détecté dans l'image {imagePath}. Elle sera ignorée.\")\n        continue\n    face = faces[0]\n    face_filename = os.path.splitext(imagePath)[0] + \"_face.jpg\"\n    cv2.imwrite(face_filename, face)\nprint(\"Extraction des visages terminée.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T11:53:04.420790Z","iopub.execute_input":"2025-04-18T11:53:04.421114Z","iopub.status.idle":"2025-04-18T11:59:16.360300Z","shell.execute_reply.started":"2025-04-18T11:53:04.421093Z","shell.execute_reply":"2025-04-18T11:59:16.359453Z"}},"outputs":[{"name":"stdout","text":"Erreur : Aucun visage détecté dans l'image data/Elisee/Elisee_0037.jpg. Elle sera ignorée.\nErreur : Aucun visage détecté dans l'image data/Elisee/Elisee_0019.png. Elle sera ignorée.\nExtraction des visages terminée.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Ou bien :\n# Charger les visages déjà extraits (pour gagner du temps)\nsuppr_data()\n\n!pip install -q gdown\nimport gdown\nfile_id = \"1OXezr6FgQIitVeKSzHaqtr4WUIQfOwIM\"\nurl = f\"https://drive.google.com/uc?id={file_id}\"\ngdown.download(url, output=\"data_faces.zip\", quiet=False)\n!unzip -q data_faces.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:28:31.517398Z","iopub.execute_input":"2025-04-18T12:28:31.517796Z","iopub.status.idle":"2025-04-18T12:28:46.407301Z","shell.execute_reply.started":"2025-04-18T12:28:31.517748Z","shell.execute_reply":"2025-04-18T12:28:46.406440Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1OXezr6FgQIitVeKSzHaqtr4WUIQfOwIM\nTo: /kaggle/working/data_faces.zip\n100%|██████████| 5.82M/5.82M [00:00<00:00, 15.0MB/s]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Répartition des visages dans des répertoires d'entraînement, de validation et de test\n\n# Création des répertoires\nfor path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(path):\n        shutil.rmtree(path) # suppression des répertoires déjà existants\n    os.makedirs(path, exist_ok=True)\n# Récupération des répertoires prenom_nom dans data\nperson_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))\n               and d not in [\"train\", \"validation\", \"test\"]]\n\nfor person in person_dirs:\n    person_path = os.path.join(base_dir, person)\n\n    # Utilisation de list_images pour récupérer les images qui terminent par \"face.jpg\"\n    images = list_images(person_path, contains=\"_face.jpg\")\n\n    # Mélange aléatoire des images\n    random.shuffle(images)\n\n    # Calcul des indices de découpage pour répartir les images en trois ensembles\n    total = len(images)\n    train_count = int(total * 0.7)  # % des images dans les données d'entraînement\n    val_count = int(total * 0.15)  # % des images pour la validation\n\n    # Découpage de l'ensemble des images en trois groupes : entraînement, validation et test\n    train_images = images[:train_count]\n    val_images = images[train_count:train_count + val_count]\n    test_images = images[train_count + val_count:]\n\n    # Définition des chemins de destination pour chaque ensemble\n    person_train_dir = os.path.join(train_dir, person)\n    person_val_dir = os.path.join(val_dir, person)\n    person_test_dir = os.path.join(test_dir, person)\n\n    # Création des répertoires de destination s'ils n'existent pas déjà\n    for path in [person_train_dir, person_val_dir, person_test_dir]:\n        os.makedirs(path, exist_ok=True)\n\n    # Déplacement des images dans les bons répertoires\n    for img in train_images:\n        shutil.move(img, os.path.join(person_train_dir, os.path.basename(img)))\n    for img in val_images:\n        shutil.move(img, os.path.join(person_val_dir, os.path.basename(img)))\n    for img in test_images:\n        shutil.move(img, os.path.join(person_test_dir, os.path.basename(img)))\n\nprint(\"Répartition des visages dans les répertoires train, validation et test terminée.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:33:44.083793Z","iopub.execute_input":"2025-04-18T12:33:44.084195Z","iopub.status.idle":"2025-04-18T12:33:44.120061Z","shell.execute_reply.started":"2025-04-18T12:33:44.084164Z","shell.execute_reply":"2025-04-18T12:33:44.119118Z"}},"outputs":[{"name":"stdout","text":"Répartition des visages dans les répertoires train, validation et test terminée.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n## Credits\n\nThis assignment is based on Adam Geitgey's [post](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n\n---","metadata":{"id":"p2vUz-z20MRO"}}]}